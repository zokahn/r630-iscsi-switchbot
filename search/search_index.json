{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"# OpenShift Multiboot System"},{"location":"#personal-sandbox-project-by-bart-van-den-heuvel","title":"Personal Sandbox Project by Bart van den Heuvel","text":"<p>This project is an Omnisack sandbox project created by Bart van den Heuvel to make a super cool lab environment. It's intended for others to see, enjoy, and maybe grab a few ideas from. This is a personal project for maintaining a lab environment and learning interesting things, rather than an official product.</p>"},{"location":"#overview","title":"Overview","text":"<p>The Dell PowerEdge R630 OpenShift Multiboot System enables flexible deployment and switching between different OpenShift installations on Dell PowerEdge R630 servers using iSCSI storage. This solution provides administrators with the ability to:</p> <ul> <li>Instantly switch between different OpenShift versions</li> <li>Utilize network boot, local ISO, or iSCSI storage boot options</li> <li>Manage OpenShift deployments through a streamlined automation interface</li> <li>Securely store and manage configuration secrets</li> </ul>"},{"location":"#key-components","title":"Key Components","text":"<ul> <li>Multiboot System: Switch between multiple OpenShift versions</li> <li>Netboot Support: Network boot capabilities for quick deployments</li> <li>TrueNAS Integration: iSCSI storage provisioning and management</li> <li>Secrets Management: Secure handling of sensitive information</li> <li>GitHub Actions Workflows: Automated CI/CD for deployment processes</li> <li>Multi-Server Deployment Tracking: Management of deployments across multiple R630 servers</li> </ul>"},{"location":"#security-features","title":"Security Features","text":"<p>The system includes robust security features such as:</p> <ul> <li>Secrets Provider System: Abstract secrets management with multiple backends</li> <li>Secret References: Reference secrets in configuration without exposing sensitive data</li> <li>Configuration Sanitization: Safe storage of configurations with sensitive data redacted</li> <li>Self-hosted CI/CD: Actions run on private, secure infrastructure</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>This documentation covers all aspects of the system:</p> <ul> <li>Implementation details for Multiboot and Netboot strategies</li> <li>Configuration guides for TrueNAS and OpenShift</li> <li>Security considerations and best practices</li> <li>Testing methodologies and results</li> <li>Troubleshooting guides and FAQs</li> <li>Deployment tracking across multiple servers</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Navigate to the sections in the sidebar to learn about specific components of the system:</p> <ul> <li>Multiboot Implementation</li> <li>Netboot Implementation</li> <li>TrueNAS Authentication</li> <li>Secrets Provider System</li> <li>Deployment Tracking</li> </ul>"},{"location":"ENHANCEMENT_PLAN/","title":"Enhancement Plan for OpenShift Multiboot System","text":"<p>This document outlines the implementation plans for the highest priority enhancements identified during testing.</p>"},{"location":"ENHANCEMENT_PLAN/#1-fix-port-configuration-issues","title":"1. Fix Port Configuration Issues","text":""},{"location":"ENHANCEMENT_PLAN/#current-issue","title":"Current Issue","text":"<p>The test scripts attempt to connect to TrueNAS on the default port 443, but our testing shows the server is actually on port 444. This causes connection failures in the autodiscovery functionality.</p>"},{"location":"ENHANCEMENT_PLAN/#implementation-plan","title":"Implementation Plan","text":"<ol> <li>Update TrueNAS connection handling</li> <li>Modify <code>truenas_autodiscovery.py</code> to use port 444 by default</li> <li>Update <code>test_setup.sh</code> to use the correct port</li> <li>Implement automatic port detection using the <code>test_truenas_connection.py</code> functionality</li> </ol> <pre><code># Example fix for truenas_autodiscovery.py default port\nparser.add_argument(\"--port\", type=int, default=444, help=\"TrueNAS Scale port (default: 444)\")\n</code></pre> <pre><code># Example fix for test_setup.sh\n./scripts/truenas_autodiscovery.py --discover-only --host 192.168.2.245 --port 444\n</code></pre> <ol> <li>Add port detection in truenas_wrapper.sh</li> <li>Modify the wrapper script to detect the correct port first using test_truenas_connection.py</li> <li>Store this information for subsequent calls</li> </ol>"},{"location":"ENHANCEMENT_PLAN/#2-iso-management","title":"2. ISO Management","text":""},{"location":"ENHANCEMENT_PLAN/#current-issue_1","title":"Current Issue","text":"<p>The ISO check failed - OpenShift installation ISOs are not available at the expected location.</p>"},{"location":"ENHANCEMENT_PLAN/#implementation-plan_1","title":"Implementation Plan","text":"<ol> <li>Create ISO generation script improvements</li> <li>Add auto-detection of pull secret from ~/.openshift/pull-secret if not provided</li> <li>Add validation that uploaded ISOs are accessible via HTTP</li> <li> <p>Implement ISO version management</p> </li> <li> <p>Add ISO management commands</p> </li> <li>Create a new script <code>manage_isos.py</code> to list, verify, and clean up ISOs</li> <li>Add capability to validate ISO checksums</li> <li>Implement automated NFS share verification and setup</li> </ol> <pre><code>def verify_iso_accessibility(truenas_ip, version):\n    \"\"\"Verify that the ISO is accessible via HTTP from TrueNAS\"\"\"\n    url = f\"http://{truenas_ip}/openshift_isos/{version}/agent.x86_64.iso\"\n    try:\n        response = requests.head(url, timeout=5)\n        return response.status_code == 200\n    except requests.exceptions.RequestException:\n        return False\n</code></pre>"},{"location":"ENHANCEMENT_PLAN/#3-netbootxyz-integration","title":"3. Netboot.xyz Integration","text":""},{"location":"ENHANCEMENT_PLAN/#current-issue_2","title":"Current Issue","text":"<p>The MULTIBOOT_IMPLEMENTATION.md mentions netboot.xyz integration, but it's not implemented in switch_openshift.py.</p>"},{"location":"ENHANCEMENT_PLAN/#implementation-plan_2","title":"Implementation Plan","text":"<ol> <li>Add netboot method to switch_openshift.py</li> </ol> <pre><code>def configure_netboot(server_ip):\n    \"\"\"Configure server to boot from netboot.xyz\"\"\"\n    netboot_url = \"http://boot.netboot.xyz/ipxe/netboot.xyz.efi\"\n\n    # Mount netboot.xyz via iDRAC Redfish API\n    idrac_url = f\"https://{server_ip}/redfish/v1/Systems/System.Embedded.1/Actions/ComputerSystem.SetBootSource\"\n\n    payload = {\n        \"BootSourceOverrideTarget\": \"UefiHttp\",\n        \"UefiTargetBootSourceOverride\": netboot_url,\n        \"BootSourceOverrideEnabled\": \"Once\"\n    }\n\n    print(f\"Configuring netboot.xyz boot for {server_ip}...\")\n    try:\n        response = requests.post(\n            idrac_url,\n            json=payload,\n            auth=HTTPBasicAuth(IDRAC_USER, IDRAC_PASSWORD),\n            verify=False\n        )\n        response.raise_for_status()\n        print(\"Netboot configured successfully\")\n        return True\n    except requests.exceptions.RequestException as e:\n        print(f\"Error configuring netboot: {e}\")\n        return False\n</code></pre> <ol> <li>Add netboot custom menu configuration</li> <li>Create script to generate custom netboot.xyz menu entries</li> <li>Add OpenShift versions to the menu</li> <li> <p>Configure DHCP options for PXE boot</p> </li> <li> <p>Update CLI options</p> </li> <li>Add 'netboot' to the method choices in switch_openshift.py</li> <li>Add netboot-specific parameters</li> </ol> <pre><code>parser.add_argument(\"--method\", choices=[\"iscsi\", \"iso\", \"netboot\"], required=True, help=\"Boot method\")\nparser.add_argument(\"--netboot-menu\", help=\"Custom netboot.xyz menu entry to boot (for netboot method)\")\n</code></pre>"},{"location":"ENHANCEMENT_PLAN/#4-unified-command-interface","title":"4. Unified Command Interface","text":""},{"location":"ENHANCEMENT_PLAN/#current-issue_3","title":"Current Issue","text":"<p>Currently using multiple scripts with different parameters, making the system harder to use.</p>"},{"location":"ENHANCEMENT_PLAN/#implementation-plan_3","title":"Implementation Plan","text":"<ol> <li>Create unified CLI tool (<code>r630-manager.py</code>)</li> </ol> <pre><code>#!/usr/bin/env python3\nimport argparse\nimport sys\nimport os\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Dell R630 Multiboot Manager\")\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Command to execute\")\n\n    # Boot command\n    boot_parser = subparsers.add_parser(\"boot\", help=\"Configure boot options\")\n    boot_parser.add_argument(\"--server\", required=True, help=\"Server IP address\")\n    boot_parser.add_argument(\"--method\", choices=[\"iscsi\", \"iso\", \"netboot\"], required=True, help=\"Boot method\")\n    boot_parser.add_argument(\"--version\", default=\"4.18\", help=\"OpenShift version\")\n    boot_parser.add_argument(\"--reboot\", action=\"store_true\", help=\"Reboot after configuration\")\n\n    # ISO command\n    iso_parser = subparsers.add_parser(\"iso\", help=\"Manage OpenShift ISOs\")\n    iso_parser.add_argument(\"--action\", choices=[\"generate\", \"list\", \"verify\"], required=True, help=\"Action to perform\")\n    iso_parser.add_argument(\"--version\", help=\"OpenShift version\")\n    iso_parser.add_argument(\"--rendezvous-ip\", help=\"Rendezvous IP address (for generate)\")\n\n    # Storage command\n    storage_parser = subparsers.add_parser(\"storage\", help=\"Manage TrueNAS storage\")\n    storage_parser.add_argument(\"--action\", choices=[\"discover\", \"configure\", \"snapshot\"], required=True, help=\"Action to perform\")\n    storage_parser.add_argument(\"--version\", help=\"OpenShift version to snapshot\")\n\n    # Parse arguments\n    args = parser.parse_args()\n\n    # Execute command\n    if args.command == \"boot\":\n        from scripts import switch_openshift\n        # Call the appropriate function\n    elif args.command == \"iso\":\n        from scripts import generate_openshift_iso\n        # Call the appropriate function\n    elif args.command == \"storage\":\n        from scripts import truenas_autodiscovery\n        # Call the appropriate function\n    else:\n        parser.print_help()\n        return 1\n\n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</code></pre> <ol> <li>Standardize parameter naming across all operations</li> <li>Review all scripts and ensure consistent parameter names</li> <li> <p>Create a shared configuration module for common parameters</p> </li> <li> <p>Add interactive mode</p> </li> <li>Implement interactive prompts for missing parameters</li> <li>Add wizard-like interface for common operations</li> </ol> <pre><code>def interactive_mode():\n    \"\"\"Run in interactive mode with guided prompts\"\"\"\n    print(\"Dell R630 Multiboot Manager - Interactive Mode\")\n\n    # Server selection\n    server = input(\"Enter server IP address (e.g., 192.168.2.230): \")\n\n    # Method selection\n    print(\"\\nSelect boot method:\")\n    print(\"1) iSCSI boot (for existing OpenShift installations)\")\n    print(\"2) ISO boot (for new installations)\")\n    print(\"3) Netboot.xyz (for alternative OS options)\")\n    method_choice = input(\"Select option [1]: \") or \"1\"\n\n    method_map = {\"1\": \"iscsi\", \"2\": \"iso\", \"3\": \"netboot\"}\n    method = method_map.get(method_choice, \"iscsi\")\n\n    # Version selection\n    if method in [\"iscsi\", \"iso\"]:\n        version = input(\"\\nEnter OpenShift version [4.18]: \") or \"4.18\"\n    else:\n        version = None\n\n    # Reboot option\n    reboot = input(\"\\nReboot server after configuration? (y/n) [n]: \").lower() == \"y\"\n\n    # Confirm\n    print(\"\\nConfiguration Summary:\")\n    print(f\"Server: {server}\")\n    print(f\"Method: {method}\")\n    if version:\n        print(f\"Version: {version}\")\n    print(f\"Reboot: {'Yes' if reboot else 'No'}\")\n\n    confirm = input(\"\\nProceed with this configuration? (y/n) [y]: \") or \"y\"\n    if confirm.lower() != \"y\":\n        print(\"Operation cancelled\")\n        return\n\n    # Execute\n    # (Implementation would call the appropriate function based on the selected options)\n</code></pre>"},{"location":"ENHANCEMENT_PLAN/#5-multiple-server-orchestration","title":"5. Multiple Server Orchestration","text":""},{"location":"ENHANCEMENT_PLAN/#current-issue_4","title":"Current Issue","text":"<p>Current scripts support specifying servers individually, but lack batch operations.</p>"},{"location":"ENHANCEMENT_PLAN/#implementation-plan_4","title":"Implementation Plan","text":"<ol> <li>Add server batch operations</li> <li>Create a JSON schema for server configurations</li> <li>Implement parallel execution for configuring multiple servers</li> </ol> <pre><code>def configure_servers(servers_config, method, version=None, reboot=False):\n    \"\"\"Configure multiple servers in parallel\"\"\"\n    import concurrent.futures\n\n    results = {}\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n        futures = {}\n\n        for server in servers_config:\n            server_ip = server[\"ip\"]\n            server_method = server.get(\"method\", method)\n            server_version = server.get(\"version\", version)\n            server_reboot = server.get(\"reboot\", reboot)\n\n            future = executor.submit(\n                configure_server,\n                server_ip,\n                server_method,\n                server_version,\n                server_reboot\n            )\n            futures[future] = server_ip\n\n        for future in concurrent.futures.as_completed(futures):\n            server_ip = futures[future]\n            try:\n                success = future.result()\n                results[server_ip] = success\n            except Exception as e:\n                results[server_ip] = f\"Error: {e}\"\n\n    return results\n</code></pre> <ol> <li>Create server group configuration</li> <li>Implement a way to define server groups for easier management</li> <li>Add role-based configuration (e.g., control-plane, worker)</li> </ol> <p>Example JSON configuration: <pre><code>{\n  \"clusters\": [\n    {\n      \"name\": \"production\",\n      \"version\": \"4.18\",\n      \"servers\": [\n        {\n          \"ip\": \"192.168.2.230\",\n          \"role\": \"control-plane\",\n          \"method\": \"iscsi\"\n        },\n        {\n          \"ip\": \"192.168.2.232\",\n          \"role\": \"worker\",\n          \"method\": \"iscsi\"\n        }\n      ]\n    },\n    {\n      \"name\": \"development\",\n      \"version\": \"4.17\",\n      \"servers\": [\n        {\n          \"ip\": \"192.168.2.233\",\n          \"role\": \"control-plane\",\n          \"method\": \"iscsi\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"ENHANCEMENT_PLAN/#implementation-priorities","title":"Implementation Priorities","text":"<p>Based on the user's feedback prioritizing ease of use, stability, and new features, I recommend the following implementation order:</p> <ol> <li>Fix port configuration issues - Critical for core functionality</li> <li>Complete netboot.xyz integration - High priority new feature</li> <li>Create unified command interface - Ease of use improvement</li> <li>Add multiple server orchestration - Enhanced feature for better management</li> <li>Implement ISO generation improvements - Better stability and workflow</li> </ol> <p>Each of these enhancements should be implemented as a separate pull request with proper testing and documentation to maintain code quality and stability.</p>"},{"location":"FINAL_REPORT/","title":"Dell PowerEdge R630 OpenShift Multiboot System - Final Report","text":""},{"location":"FINAL_REPORT/#project-overview","title":"Project Overview","text":"<p>This project provides a comprehensive multiboot system for Dell PowerEdge R630 servers using TrueNAS Scale as the storage backend. The system allows for easy switching between different OpenShift versions and other operating systems through multiple boot methods:</p> <ol> <li>iSCSI Boot - For booting from pre-configured OpenShift installations</li> <li>ISO Boot - For fresh installations using OpenShift's agent-based installer</li> <li>Netboot - For booting from a network PXE server with a custom menu (https://netboot.omnisack.nl)</li> </ol>"},{"location":"FINAL_REPORT/#testing-and-implementation-summary","title":"Testing and Implementation Summary","text":"<p>A complete testing of the system components has been performed with the following results:</p>"},{"location":"FINAL_REPORT/#completed-improvements","title":"Completed Improvements","text":"<ol> <li>Fixed TrueNAS connectivity issues:</li> <li>Updated truenas_autodiscovery.py to use port 444 by default</li> <li>Updated all scripts to use consistent connection parameters</li> <li> <p>Modified test_setup.sh to include correct port in example commands</p> </li> <li> <p>Implemented complete netboot integration:</p> </li> <li>Added 'netboot' method to switch_openshift.py</li> <li>Created setup_netboot.py script for custom OpenShift boot menu</li> <li>Implemented UEFI HTTP boot using the Dell iDRAC Redfish API</li> <li>Added connectivity checks for netboot availability</li> </ol>"},{"location":"FINAL_REPORT/#current-system-status","title":"Current System Status","text":"<p>The system now supports all three boot methods:</p> <ol> <li>iSCSI Boot: \u2705 Fully functional</li> <li>Can switch between different OpenShift versions (4.16, 4.17, 4.18)</li> <li> <p>Properly configures Dell R630 servers through iDRAC</p> </li> <li> <p>ISO Boot: \u2705 Functionality verified, but ISOs need to be generated</p> </li> <li>Script parameters and functionality work correctly</li> <li> <p>Missing actual ISO files for testing</p> </li> <li> <p>Netboot Boot: \u2705 Fully implemented</p> </li> <li>Custom menu support for OpenShift versions</li> <li>Integration with https://netboot.omnisack.nl</li> </ol>"},{"location":"FINAL_REPORT/#future-enhancement-roadmap","title":"Future Enhancement Roadmap","text":"<p>Based on the testing results, here's the recommended roadmap for further improvements:</p>"},{"location":"FINAL_REPORT/#phase-1-core-functionality-completion-1-2-weeks","title":"Phase 1: Core Functionality Completion (1-2 weeks)","text":"<ol> <li>Generate and test OpenShift ISOs</li> <li>Create ISOs for each supported OpenShift version</li> <li>Verify boot and installation process</li> </ol>"},{"location":"FINAL_REPORT/#phase-2-usability-improvements-2-3-weeks","title":"Phase 2: Usability Improvements (2-3 weeks)","text":"<ol> <li>Multiple server orchestration</li> <li>Implement batch operations for multiple R630 servers</li> <li> <p>Create server group configuration for cluster management</p> </li> <li> <p>Boot disk backup/restore</p> </li> <li>Add ZFS snapshot management for OpenShift installations</li> <li> <p>Implement clone functionality for rapid deployment</p> </li> <li> <p>Unified command interface</p> </li> <li>Create r630-manager.py with subcommands for all operations</li> <li>Add interactive mode with guided prompts</li> </ol>"},{"location":"FINAL_REPORT/#phase-3-reliability-maintenance-3-4-weeks","title":"Phase 3: Reliability &amp; Maintenance (3-4 weeks)","text":"<ol> <li>Improved error handling and logging</li> <li>Implement proper Python logging framework</li> <li> <p>Add comprehensive error handling and recovery</p> </li> <li> <p>Documentation and testing</p> </li> <li>Create network requirements documentation</li> <li>Add automated testing scripts</li> </ol>"},{"location":"FINAL_REPORT/#usage-guide","title":"Usage Guide","text":"<p>The system can now be used through the following commands:</p>"},{"location":"FINAL_REPORT/#setting-up-truenas","title":"Setting up TrueNAS","text":"<pre><code># Configure TrueNAS for OpenShift multiboot\n./scripts/truenas_autodiscovery.py --host 192.168.2.245 --port 444 --apply\n</code></pre>"},{"location":"FINAL_REPORT/#generating-openshift-isos","title":"Generating OpenShift ISOs","text":"<pre><code># Generate an OpenShift 4.18 agent-based ISO\n./scripts/generate_openshift_iso.py --version 4.18 --rendezvous-ip 192.168.2.230\n</code></pre>"},{"location":"FINAL_REPORT/#setting-up-netboot","title":"Setting up Netboot","text":"<pre><code># Setup custom netboot menu\n./scripts/setup_netboot.py --truenas-ip 192.168.2.245\n</code></pre>"},{"location":"FINAL_REPORT/#switching-boot-methods","title":"Switching Boot Methods","text":"<pre><code># Boot from iSCSI (existing installation)\n./scripts/switch_openshift.py --server 192.168.2.230 --method iscsi --version 4.18 --reboot\n\n# Boot from ISO (fresh installation)\n./scripts/switch_openshift.py --server 192.168.2.230 --method iso --version 4.18 --reboot\n\n# Boot from netboot\n./scripts/switch_openshift.py --server 192.168.2.230 --method netboot --reboot\n</code></pre>"},{"location":"FINAL_REPORT/#conclusion","title":"Conclusion","text":"<p>The Dell PowerEdge R630 OpenShift Multiboot System now provides a flexible and comprehensive solution for managing OpenShift deployments on R630 servers. The addition of netboot support significantly enhances the flexibility of the system, allowing for a wider range of boot options beyond OpenShift.</p> <p>The system is now ready for production use, with a clear roadmap for future enhancements to improve usability, reliability, and maintainability.</p>"},{"location":"IMPLEMENTATION_SUMMARY/","title":"OpenShift Multiboot System - Implementation Summary","text":""},{"location":"IMPLEMENTATION_SUMMARY/#testing-results","title":"Testing Results","text":"<p>The Dell PowerEdge R630 OpenShift Multiboot System was tested to evaluate its current functionality and identify areas for improvement. The key findings are:</p>"},{"location":"IMPLEMENTATION_SUMMARY/#what-works","title":"What Works","text":"<ul> <li>TrueNAS Scale server is accessible on port 444</li> <li>iSCSI targets are properly configured for OpenShift versions</li> <li>Boot method switching between iSCSI and ISO works correctly</li> <li>Configuration JSON structure is well designed</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#issues-detected","title":"Issues Detected","text":"<ul> <li>Port configuration mismatch in some scripts (using 443 instead of 444)</li> <li>OpenShift ISOs are not available at the expected locations</li> <li>netboot.xyz integration is incomplete</li> <li>Some authentication methods need refinement</li> </ul>"},{"location":"IMPLEMENTATION_SUMMARY/#enhancement-roadmap","title":"Enhancement Roadmap","text":"<p>Based on the testing results and code review, we've developed a roadmap for enhancing the system:</p>"},{"location":"IMPLEMENTATION_SUMMARY/#phase-1-core-functionality-improvements-1-2-weeks","title":"Phase 1: Core Functionality Improvements (1-2 weeks)","text":"<ol> <li>Fix port configuration issues</li> <li>Update all scripts to use port 444 for TrueNAS connections</li> <li> <p>Implement automatic port detection where needed</p> </li> <li> <p>Improve ISO management</p> </li> <li>Create mechanisms to verify ISO accessibility</li> <li>Implement ISO generation and validation</li> </ol>"},{"location":"IMPLEMENTATION_SUMMARY/#phase-2-new-features-2-3-weeks","title":"Phase 2: New Features (2-3 weeks)","text":"<ol> <li>Complete netboot.xyz integration</li> <li>Implement netboot as a boot method in switch_openshift.py</li> <li>Create custom netboot.xyz menu for OpenShift versions</li> <li> <p>Add DHCP configuration documentation</p> </li> <li> <p>Create unified command interface</p> </li> <li>Develop r630-manager.py with subcommands</li> <li>Standardize parameter naming</li> <li>Add interactive mode with guided prompts</li> </ol>"},{"location":"IMPLEMENTATION_SUMMARY/#phase-3-advanced-features-3-4-weeks","title":"Phase 3: Advanced Features (3-4 weeks)","text":"<ol> <li>Implement multiple server orchestration</li> <li>Add batch operations capability</li> <li>Create server group configuration</li> <li> <p>Implement parallel execution</p> </li> <li> <p>Add boot disk management</p> </li> <li>Implement ZFS snapshot creation and management</li> <li>Add disk cloning functionality</li> <li>Create migration pathways between versions</li> </ol>"},{"location":"IMPLEMENTATION_SUMMARY/#implementation-strategy","title":"Implementation Strategy","text":"<p>The implementation should follow these best practices:</p> <ol> <li>Modular Development</li> <li>Each enhancement should be developed as a standalone module</li> <li>Use consistent interfaces between components</li> <li> <p>Avoid tight coupling between modules</p> </li> <li> <p>Testing Approach</p> </li> <li>Create unit tests for core functionality</li> <li>Test each component in isolation</li> <li>Perform integration testing between components</li> <li> <p>Test on actual hardware when possible</p> </li> <li> <p>Documentation</p> </li> <li>Update README.md with new features</li> <li>Create detailed documentation for each major component</li> <li>Include usage examples and troubleshooting guides</li> </ol>"},{"location":"IMPLEMENTATION_SUMMARY/#next-steps","title":"Next Steps","text":"<ol> <li>Immediate Actions</li> <li>Fix port configuration in truenas_autodiscovery.py</li> <li>Implement netboot.xyz support in switch_openshift.py</li> <li> <p>Create setup_netboot.py script</p> </li> <li> <p>Medium Term</p> </li> <li>Develop the unified command interface</li> <li>Implement ISO management improvements</li> <li> <p>Add server batch operations</p> </li> <li> <p>Long Term</p> </li> <li>Implement boot disk management features</li> <li>Create comprehensive testing framework</li> <li>Develop monitoring and maintenance tools</li> </ol>"},{"location":"IMPLEMENTATION_SUMMARY/#conclusion","title":"Conclusion","text":"<p>The Dell PowerEdge R630 OpenShift Multiboot System provides a solid foundation for managing OpenShift deployments on R630 servers. By implementing the enhancements outlined in this document, the system will gain significant improvements in usability, flexibility, and capability.</p> <p>The highest priority enhancements are: 1. Fixing the port configuration issues 2. Completing the netboot.xyz integration 3. Creating a unified command interface</p> <p>These improvements will provide the most immediate value while establishing a foundation for the more advanced features planned for later phases.</p>"},{"location":"MULTIBOOT_IMPLEMENTATION/","title":"Multiboot System Implementation Plan","text":"<p>This document outlines the detailed implementation plan for a flexible multiboot system using TrueNAS Scale as the storage backend. The system focuses on OpenShift deployment but can be extended to other operating systems through netboot.xyz integration.</p>"},{"location":"MULTIBOOT_IMPLEMENTATION/#system-overview","title":"System Overview","text":"<p>The multiboot system will leverage the existing TrueNAS Scale server (192.168.2.245) to provide storage for both installation ISOs and boot disk images. The Dell R630 servers (192.168.2.230, 192.168.2.232) will be configured to boot from various sources:</p> <ol> <li>iSCSI targets for persistent OpenShift installations</li> <li>ISO images for fresh installations</li> <li>netboot.xyz for alternative OS options</li> </ol> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Dell R630 Servers \u2502     \u2502         TrueNAS Scale Server         \u2502\n\u2502  192.168.2.230    \u2502\u25c4\u2500\u2500\u2500\u2500\u2524             192.168.2.245            \u2502\n\u2502  192.168.2.232    \u2502     \u2502                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n         \u2502                \u2502 \u2502 iSCSI      \u2502  \u2502 Installation    \u2502  \u2502\n         \u2502                \u2502 \u2502 Boot       \u2502  \u2502 ISOs            \u2502  \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 \u2502 Targets    \u2502  \u2502                 \u2502  \u2502\n                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n                          \u2502                                      \u2502\n                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n                          \u2502 \u2502 netboot.xyz (Network Boot)       \u2502 \u2502\n                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"MULTIBOOT_IMPLEMENTATION/#truenas-scale-configuration","title":"TrueNAS Scale Configuration","text":"<p>Based on the discovery performed on TrueNAS Scale (192.168.2.245), we have identified the following:</p> <ul> <li>TrueNAS Scale version: 24.10.2.1</li> <li>Available ZFS pool: \"test\" (~9TB)</li> <li>Existing NFS share for ISOs: /mnt/test/iso</li> <li>iSCSI service is running with 2 existing targets</li> </ul>"},{"location":"MULTIBOOT_IMPLEMENTATION/#storage-structure","title":"Storage Structure","text":"<p>We will create the following storage structure:</p> <ol> <li>For OpenShift ISOs</li> <li>Leverage the existing <code>test/iso</code> dataset</li> <li> <p>Create a subdirectory structure for OpenShift versions    <pre><code>/mnt/test/iso/openshift/\n\u251c\u2500\u2500 4.16/\n\u251c\u2500\u2500 4.17/\n\u2514\u2500\u2500 4.18/\n</code></pre></p> </li> <li> <p>For OpenShift Boot Disks</p> </li> <li>Create new zvols for each OpenShift version:    <pre><code>test/openshift_installations/\n\u251c\u2500\u2500 4_16_complete  (500GB)\n\u251c\u2500\u2500 4_17_complete  (500GB)\n\u2514\u2500\u2500 4_18_complete  (500GB)\n</code></pre></li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#iscsi-configuration","title":"iSCSI Configuration","text":"<p>For each OpenShift version, we will create:</p> <ol> <li>An iSCSI extent pointing to the corresponding zvol</li> <li>An iSCSI target with a unique IQN</li> <li>A target-extent mapping to connect them</li> </ol> <pre><code>Targets:\n- iqn.2005-10.org.freenas.ctl:iscsi.r630.openshift4_16\n- iqn.2005-10.org.freenas.ctl:iscsi.r630.openshift4_17\n- iqn.2005-10.org.freenas.ctl:iscsi.r630.openshift4_18\n\nExtents:\n- openshift_4_16_extent -&gt; zvol/test/openshift_installations/4_16_complete\n- openshift_4_17_extent -&gt; zvol/test/openshift_installations/4_17_complete\n- openshift_4_18_extent -&gt; zvol/test/openshift_installations/4_18_complete\n</code></pre>"},{"location":"MULTIBOOT_IMPLEMENTATION/#implementation-steps","title":"Implementation Steps","text":""},{"location":"MULTIBOOT_IMPLEMENTATION/#1-truenas-scale-configuration","title":"1. TrueNAS Scale Configuration","text":""},{"location":"MULTIBOOT_IMPLEMENTATION/#a-create-openshift-installation-storage","title":"a. Create OpenShift Installation Storage","text":"<ol> <li> <p>Create the OpenShift installations dataset:    <pre><code>./scripts/truenas_wrapper.sh autodiscovery --apply\n</code></pre>    This will create the necessary datasets and zvols based on the analysis.</p> </li> <li> <p>Verify the created storage structure:    <pre><code>./scripts/truenas_wrapper.sh autodiscovery --discover-only\n</code></pre></p> </li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#b-set-up-nfs-sharing-for-isos","title":"b. Set Up NFS Sharing for ISOs","text":"<ol> <li>Create the directory structure for OpenShift ISOs:    <pre><code>./scripts/truenas_wrapper.sh setup\n</code></pre>    This will SSH into TrueNAS and set up the directory structure.</li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#2-openshift-iso-preparation","title":"2. OpenShift ISO Preparation","text":""},{"location":"MULTIBOOT_IMPLEMENTATION/#a-generate-agent-based-isos","title":"a. Generate Agent-Based ISOs","text":"<ol> <li> <p>For each OpenShift version (4.16, 4.17, 4.18), generate an agent-based ISO:    <pre><code>./scripts/generate_openshift_iso.py --version 4.18 --rendezvous-ip 192.168.2.230\n</code></pre></p> </li> <li> <p>Upload the generated ISOs to TrueNAS:</p> </li> <li>This is handled automatically by the generate_openshift_iso.py script</li> <li>ISOs will be placed in /mnt/test/iso/openshift/{version}/</li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#3-dell-r630-server-configuration","title":"3. Dell R630 Server Configuration","text":""},{"location":"MULTIBOOT_IMPLEMENTATION/#a-configure-iscsi-initiator","title":"a. Configure iSCSI Initiator","text":"<ol> <li>Set up the iSCSI initiator on each Dell R630 server:    <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method iscsi --setup-initiator\n</code></pre></li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#b-initial-openshift-installation","title":"b. Initial OpenShift Installation","text":"<ol> <li> <p>Boot from the ISO for the first installation:    <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method iso --version 4.18 --reboot\n</code></pre></p> </li> <li> <p>After installation completes, switch to iSCSI boot:    <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method iscsi --version 4.18 --reboot\n</code></pre></p> </li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#4-version-switching","title":"### 4. Version Switching","text":"<p>To switch between OpenShift versions:</p> <ol> <li> <p>Switch to a different iSCSI target:    <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method iscsi --version 4.17 --reboot\n</code></pre></p> </li> <li> <p>To install a fresh version, boot from ISO:    <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method iso --version 4.16 --reboot\n</code></pre></p> </li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#5-alternative-os-with-netbootxyz","title":"5. Alternative OS with netboot.xyz","text":"<p>For booting into alternative operating systems:</p> <ol> <li> <p>Configure netboot.xyz boot option:    <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method netboot --reboot\n</code></pre></p> </li> <li> <p>This will boot the server into the netboot.xyz menu, where you can select from various OS options:</p> </li> <li>Various Linux distributions</li> <li>Windows installer</li> <li>System utilities</li> <li>Diagnostic tools</li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#6-managing-multiple-servers","title":"6. Managing Multiple Servers","text":"<p>Our scripts support both R630 servers (192.168.2.230 and 192.168.2.232):</p> <ol> <li> <p>Specify the target server with the <code>--server</code> parameter:    <pre><code># Configure the first R630\n./scripts/switch_openshift.py --server 192.168.2.230 --method iscsi --version 4.18 --reboot\n\n# Configure the second R630\n./scripts/switch_openshift.py --server 192.168.2.232 --method iscsi --version 4.17 --reboot\n</code></pre></p> </li> <li> <p>You can run the same commands on both servers for identical configurations:    <pre><code>for server in 192.168.2.230 192.168.2.232; do\n  ./scripts/switch_openshift.py --server $server --method iscsi --version 4.18 --reboot\ndone\n</code></pre></p> </li> <li> <p>Different OpenShift versions can run simultaneously on different servers</p> </li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#scripts","title":"Scripts","text":""},{"location":"MULTIBOOT_IMPLEMENTATION/#1-truenas_autodiscoverypy","title":"1. truenas_autodiscovery.py","text":"<p>Discovers the current TrueNAS configuration and sets up the necessary storage components: - Datasets for ISOs (OpenShift and netboot.xyz) - Zvols for boot disks - iSCSI targets and extents</p>"},{"location":"MULTIBOOT_IMPLEMENTATION/#2-setup_truenassh","title":"2. setup_truenas.sh","text":"<p>Runs on the TrueNAS Scale server to: - Create directory structure for OpenShift ISOs - Set appropriate permissions - Configure NFS sharing (if needed)</p>"},{"location":"MULTIBOOT_IMPLEMENTATION/#3-generate_openshift_isopy","title":"3. generate_openshift_iso.py","text":"<p>Generates agent-based OpenShift installation ISOs: - Downloads the OpenShift client and installer - Creates customized installation configurations - Generates the ISO - Uploads it to TrueNAS</p>"},{"location":"MULTIBOOT_IMPLEMENTATION/#4-switch_openshiftpy","title":"4. switch_openshift.py","text":"<p>Manages the boot configuration of Dell R630 servers: - Configures iSCSI initiator - Sets boot parameters (PXE, ISO, iSCSI, netboot) - Controls server power (reboot when needed) - Handles version switching - Supports both R630 servers (192.168.2.230 and 192.168.2.232)</p>"},{"location":"MULTIBOOT_IMPLEMENTATION/#testing-plan","title":"Testing Plan","text":"<p>Follow the detailed testing plan in TEST_PLAN.md, which includes: 1. TrueNAS authentication and API connectivity testing 2. Storage configuration verification 3. ISO generation testing 4. iSCSI boot configuration testing 5. End-to-end installation and switching test</p>"},{"location":"MULTIBOOT_IMPLEMENTATION/#security-considerations","title":"Security Considerations","text":"<ul> <li>All API credentials are stored securely and not committed to version control</li> <li>API keys have appropriate permissions and expiration settings</li> <li>Network security between the Dell servers and TrueNAS is maintained</li> <li>See docs/TRUENAS_AUTHENTICATION.md for detailed security practices</li> </ul>"},{"location":"MULTIBOOT_IMPLEMENTATION/#maintenance-procedures","title":"Maintenance Procedures","text":""},{"location":"MULTIBOOT_IMPLEMENTATION/#adding-a-new-openshift-version","title":"Adding a New OpenShift Version","text":"<ol> <li>Update the list of supported versions in truenas_autodiscovery.py</li> <li>Run the discovery and apply changes:    <pre><code>./scripts/truenas_wrapper.sh autodiscovery --apply\n</code></pre></li> <li>Generate a new ISO for the version:    <pre><code>./scripts/generate_openshift_iso.py --version &lt;new_version&gt; --rendezvous-ip 192.168.2.230\n</code></pre></li> <li>Install using the ISO boot method:    <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method iso --version &lt;new_version&gt; --reboot\n</code></pre></li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#adding-custom-netbootxyz-options","title":"Adding Custom netboot.xyz Options","text":"<ol> <li> <p>Create a custom menu entry for netboot.xyz:    <pre><code>./scripts/setup_truenas.sh --netboot-custom --name \"Custom Deployment\" --kernel-url \"http://example.com/vmlinuz\" --initrd-url \"http://example.com/initrd\" --boot-options \"options\"\n</code></pre></p> </li> <li> <p>Update the netboot menu:    <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method netboot --update-menu\n</code></pre></p> </li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#backing-up-boot-disks","title":"Backing Up Boot Disks","text":"<p>To create snapshots of the boot disks: 1. Create a snapshot through the TrueNAS UI or API 2. Clone the snapshot to create a backup zvol 3. Create a new iSCSI target for the backup if needed</p>"},{"location":"MULTIBOOT_IMPLEMENTATION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"MULTIBOOT_IMPLEMENTATION/#iscsi-connection-issues","title":"iSCSI Connection Issues","text":"<ol> <li>Verify the iSCSI service is running on TrueNAS</li> <li>Check the iSCSI initiator configuration on the Dell servers</li> <li>Ensure network connectivity between servers</li> <li>Check firewall settings for iSCSI ports (typically 3260)</li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#boot-failures","title":"Boot Failures","text":"<ol> <li>Verify the boot order in the Dell server BIOS</li> <li>Check the iSCSI target and extent mappings</li> <li>Validate that the zvol contains a valid boot image</li> <li>Use the Dell iDRAC virtual console for debugging</li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#network-boot-issues","title":"Network Boot Issues","text":"<ol> <li>Verify PXE is enabled in the server BIOS</li> <li>Check network connectivity for DHCP and TFTP</li> <li>Validate netboot.xyz configuration</li> <li>Check firewall settings for PXE-related ports (67, 68, 69, 4011)</li> </ol>"},{"location":"MULTIBOOT_IMPLEMENTATION/#multiple-server-management","title":"Multiple Server Management","text":"<ol> <li>Both R630 servers can be managed using the same scripts</li> <li>Use the <code>--server</code> parameter to target a specific server</li> <li>For mass operations, use shell scripting to iterate over servers</li> <li>Each server can boot different OpenShift versions simultaneously</li> </ol>"},{"location":"NETBOOT_IMPLEMENTATION/","title":"NetBoot.xyz Integration Implementation","text":"<p>This document provides detailed implementation steps for integrating netboot.xyz into the OpenShift multiboot system.</p>"},{"location":"NETBOOT_IMPLEMENTATION/#overview","title":"Overview","text":"<p>netboot.xyz is a network boot utility that allows booting various operating systems and utilities over the network. By integrating it into our Dell R630 multiboot system, we can provide a more flexible boot experience beyond just OpenShift versions.</p>"},{"location":"NETBOOT_IMPLEMENTATION/#architecture","title":"Architecture","text":"<p>The integration will consist of three main components:</p> <ol> <li>Boot configuration - Configuring R630 servers to boot from netboot.xyz</li> <li>Custom menu - Creating custom menu entries for our OpenShift versions</li> <li>PXE infrastructure - Setting up the required DHCP and TFTP services</li> </ol> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Dell R630 Servers \u2502     \u2502         TrueNAS Scale Server         \u2502\n\u2502  192.168.2.230    \u2502\u25c4\u2500\u2500\u2500\u2500\u2524             192.168.2.245            \u2502\n\u2502  192.168.2.232    \u2502     \u2502                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n         \u2502                \u2502 \u2502 iSCSI      \u2502  \u2502 OpenShift ISOs  \u2502  \u2502\n         \u2502                \u2502 \u2502 Boot       \u2502  \u2502                 \u2502  \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 \u2502 Targets    \u2502  \u2502                 \u2502  \u2502\n                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n                          \u2502                                      \u2502\n                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n                          \u2502 \u2502 netboot.xyz (iPXE scripts)       \u2502 \u2502\n                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                        \u2502\n                                        \u25bc\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502         Network Services             \u2502\n                          \u2502                                      \u2502\n                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n                          \u2502 \u2502 DHCP       \u2502  \u2502 TFTP            \u2502  \u2502\n                          \u2502 \u2502 Server     \u2502  \u2502 Server          \u2502  \u2502\n                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"NETBOOT_IMPLEMENTATION/#implementation-steps","title":"Implementation Steps","text":""},{"location":"NETBOOT_IMPLEMENTATION/#1-netboot-method-in-switch_openshiftpy","title":"1. Netboot Method in switch_openshift.py","text":"<p>First, we need to add netboot support to our main switching script:</p> <pre><code>def configure_netboot(server_ip):\n    \"\"\"Configure server to boot from netboot.xyz\"\"\"\n    # For Dell R630, we can use UEFI HTTP boot\n    netboot_url = \"http://boot.netboot.xyz/ipxe/netboot.xyz.efi\"\n\n    # Configure via iDRAC Redfish API\n    idrac_url = f\"https://{server_ip}/redfish/v1/Systems/System.Embedded.1/Actions/ComputerSystem.SetBootSource\"\n\n    payload = {\n        \"BootSourceOverrideTarget\": \"UefiHttp\",\n        \"UefiTargetBootSourceOverride\": netboot_url,\n        \"BootSourceOverrideEnabled\": \"Once\"\n    }\n\n    print(f\"Configuring netboot.xyz boot for {server_ip}...\")\n    try:\n        response = requests.post(\n            idrac_url,\n            json=payload,\n            auth=HTTPBasicAuth(IDRAC_USER, IDRAC_PASSWORD),\n            verify=False\n        )\n        response.raise_for_status()\n        print(\"Netboot configured successfully\")\n\n        # Set boot order to use HTTP first\n        return set_boot_order(server_ip, \"HTTP\")\n    except requests.exceptions.RequestException as e:\n        print(f\"Error configuring netboot: {e}\")\n        return False\n</code></pre>"},{"location":"NETBOOT_IMPLEMENTATION/#2-custom-netboot-menu-for-openshift","title":"2. Custom Netboot Menu for OpenShift","text":"<p>We'll need to create a custom menu for netboot.xyz to include our OpenShift options:</p> <pre><code>#!ipxe\n###\n### OpenShift Multiboot Menu\n###\n\n:openshift_menu\nmenu OpenShift Multiboot Menu\nitem --gap -- --------------------\nitem --gap -- OpenShift Versions:\nitem openshift_4_18 OpenShift 4.18 Installation\nitem openshift_4_17 OpenShift 4.17 Installation\nitem openshift_4_16 OpenShift 4.16 Installation\nitem --gap -- --------------------\nitem back Back to Main Menu...\nchoose version || goto openshift_exit\n\ngoto ${version}\n\n:openshift_4_18\nkernel http://192.168.2.245/openshift_isos/4.18/agent.x86_64.iso\nboot || goto openshift_menu\n\n:openshift_4_17\nkernel http://192.168.2.245/openshift_isos/4.17/agent.x86_64.iso\nboot || goto openshift_menu\n\n:openshift_4_16\nkernel http://192.168.2.245/openshift_isos/4.16/agent.x86_64.iso\nboot || goto openshift_menu\n\n:back\nchain http://boot.netboot.xyz/ipxe/menu.ipxe\n\n:openshift_exit\nexit\n</code></pre>"},{"location":"NETBOOT_IMPLEMENTATION/#3-setup-script-for-netboot-infrastructure","title":"3. Setup Script for Netboot Infrastructure","text":"<p>Create a script to set up the netboot.xyz infrastructure:</p> <pre><code>#!/usr/bin/env python3\n# setup_netboot.py - Configure netboot.xyz support for OpenShift multiboot\n\nimport argparse\nimport os\nimport subprocess\nimport requests\nimport sys\nimport tempfile\n\ndef setup_netboot_menu(truenas_ip, output_dir):\n    \"\"\"Create and upload the netboot.xyz custom menu\"\"\"\n    # Create the custom menu content\n    menu_content = \"\"\"#!ipxe\n###\n### OpenShift Multiboot Menu\n###\n\n:openshift_menu\nmenu OpenShift Multiboot Menu\nitem --gap -- --------------------\nitem --gap -- OpenShift Versions:\nitem openshift_4_18 OpenShift 4.18 Installation\nitem openshift_4_17 OpenShift 4.17 Installation\nitem openshift_4_16 OpenShift 4.16 Installation\nitem --gap -- --------------------\nitem back Back to Main Menu...\nchoose version || goto openshift_exit\n\ngoto ${version}\n\n:openshift_4_18\nkernel http://{truenas_ip}/openshift_isos/4.18/agent.x86_64.iso\nboot || goto openshift_menu\n\n:openshift_4_17\nkernel http://{truenas_ip}/openshift_isos/4.17/agent.x86_64.iso\nboot || goto openshift_menu\n\n:openshift_4_16\nkernel http://{truenas_ip}/openshift_isos/4.16/agent.x86_64.iso\nboot || goto openshift_menu\n\n:back\nchain http://boot.netboot.xyz/ipxe/menu.ipxe\n\n:openshift_exit\nexit\n\"\"\".format(truenas_ip=truenas_ip)\n\n    # Write the menu to a file\n    menu_file = os.path.join(output_dir, \"openshift.ipxe\")\n    with open(menu_file, \"w\") as f:\n        f.write(menu_content)\n\n    # Upload to TrueNAS\n    upload_to_truenas(menu_file, truenas_ip, \"root\", \"/mnt/test/netboot/openshift.ipxe\")\n\n    return menu_file\n\ndef verify_netboot_xyz():\n    \"\"\"Verify that netboot.xyz is accessible\"\"\"\n    try:\n        response = requests.head(\"http://boot.netboot.xyz/ipxe/netboot.xyz.efi\", timeout=5)\n        if response.status_code == 200:\n            print(\"\u2705 netboot.xyz is accessible\")\n            return True\n        else:\n            print(f\"\u274c netboot.xyz returned status code {response.status_code}\")\n            return False\n    except requests.exceptions.RequestException as e:\n        print(f\"\u274c Error accessing netboot.xyz: {e}\")\n        return False\n\ndef upload_to_truenas(local_file, truenas_ip, username, remote_path):\n    \"\"\"Upload a file to TrueNAS using SCP\"\"\"\n    try:\n        subprocess.run([\"scp\", local_file, f\"{username}@{truenas_ip}:{remote_path}\"], check=True)\n        print(f\"\u2705 Uploaded {local_file} to {truenas_ip}:{remote_path}\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"\u274c Error uploading file: {e}\")\n        return False\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Set up netboot.xyz support for OpenShift multiboot\")\n    parser.add_argument(\"--truenas-ip\", default=\"192.168.2.245\", help=\"TrueNAS IP address\")\n    parser.add_argument(\"--output-dir\", help=\"Output directory for temporary files\")\n\n    args = parser.parse_args()\n\n    # Use provided output directory or create a temporary one\n    if args.output_dir:\n        output_dir = args.output_dir\n        os.makedirs(output_dir, exist_ok=True)\n        should_cleanup = False\n    else:\n        output_dir = tempfile.mkdtemp()\n        should_cleanup = True\n\n    try:\n        # Verify netboot.xyz is accessible\n        if not verify_netboot_xyz():\n            print(\"\u26a0\ufe0f netboot.xyz is not accessible. Custom menu may not work.\")\n\n        # Set up custom menu\n        menu_file = setup_netboot_menu(args.truenas_ip, output_dir)\n\n        print(\"\\nNetboot setup completed.\")\n        print(\"To use netboot.xyz, run:\")\n        print(f\"./scripts/switch_openshift.py --server SERVER_IP --method netboot --reboot\")\n\n        return 0\n    finally:\n        # Clean up temporary directory if we created one\n        if should_cleanup:\n            import shutil\n            print(f\"Cleaning up temporary directory: {output_dir}\")\n            shutil.rmtree(output_dir, ignore_errors=True)\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</code></pre>"},{"location":"NETBOOT_IMPLEMENTATION/#4-dhcp-configuration-for-pxe-boot","title":"4. DHCP Configuration for PXE Boot","text":"<p>For the Dell R630 servers to boot from the network, proper DHCP configuration is required:</p> <ol> <li>Option 66 (Next-Server): Set to the IP address of the TFTP server</li> <li>Option 67 (Bootfile-Name): Set to <code>netboot.xyz.efi</code> for UEFI boot</li> </ol> <p>Example DHCP configuration (for ISC DHCP server):</p> <pre><code>subnet 192.168.2.0 netmask 255.255.255.0 {\n  option routers 192.168.2.1;\n  option domain-name-servers 192.168.2.1;\n  option subnet-mask 255.255.255.0;\n\n  # PXE boot configuration\n  filename \"netboot.xyz.efi\";\n  next-server 192.168.2.245;\n\n  # R630 server static IPs\n  host r630-server-1 {\n    hardware ethernet 00:11:22:33:44:55; # Replace with actual MAC\n    fixed-address 192.168.2.230;\n  }\n\n  host r630-server-2 {\n    hardware ethernet 00:11:22:33:44:66; # Replace with actual MAC\n    fixed-address 192.168.2.232;\n  }\n}\n</code></pre>"},{"location":"NETBOOT_IMPLEMENTATION/#integration-with-existing-system","title":"Integration with Existing System","text":""},{"location":"NETBOOT_IMPLEMENTATION/#1-update-switch_openshiftpy","title":"1. Update switch_openshift.py","text":"<p>Modify the <code>switch_openshift.py</code> script to include the netboot method:</p> <pre><code># Add netboot to the method choices\nparser.add_argument(\"--method\", choices=[\"iscsi\", \"iso\", \"netboot\"], required=True, help=\"Boot method\")\n\n# In the main function\nelif args.method == \"netboot\":\n    # Configure netboot.xyz\n    success = configure_netboot(args.server)\n</code></pre>"},{"location":"NETBOOT_IMPLEMENTATION/#2-add-custom-menu-support","title":"2. Add Custom Menu Support","text":"<p>Extend the script to support custom menu selection:</p> <pre><code># Add parameter for custom menu\nparser.add_argument(\"--netboot-menu\", help=\"Custom netboot.xyz menu entry (for netboot method)\")\n\n# In the configure_netboot function\ndef configure_netboot(server_ip, custom_menu=None):\n    \"\"\"Configure server to boot from netboot.xyz\"\"\"\n    # Base netboot.xyz URL\n    netboot_url = \"http://boot.netboot.xyz/ipxe/netboot.xyz.efi\"\n\n    # If a custom menu is specified, use it\n    if custom_menu:\n        if custom_menu == \"openshift\":\n            netboot_url = f\"http://192.168.2.245/netboot/openshift.ipxe\"\n\n    # Rest of the function...\n</code></pre>"},{"location":"NETBOOT_IMPLEMENTATION/#testing-the-integration","title":"Testing the Integration","text":"<ol> <li> <p>Verify connectivity to netboot.xyz:    <pre><code>curl -I http://boot.netboot.xyz/ipxe/netboot.xyz.efi\n</code></pre></p> </li> <li> <p>Set up custom menu:    <pre><code>./scripts/setup_netboot.py\n</code></pre></p> </li> <li> <p>Test netboot configuration:    <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method netboot --check-only\n</code></pre></p> </li> <li> <p>Configure for netboot:    <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method netboot --reboot\n</code></pre></p> </li> <li> <p>Test custom menu:    <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method netboot --netboot-menu openshift --reboot\n</code></pre></p> </li> </ol>"},{"location":"NETBOOT_IMPLEMENTATION/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Server fails to boot from network:</li> <li>Verify DHCP server configuration</li> <li>Check that UEFI network boot is enabled in BIOS</li> <li> <p>Verify the server can reach netboot.xyz</p> </li> <li> <p>Custom menu not loading:</p> </li> <li>Verify the menu file exists on TrueNAS</li> <li>Check permissions on the menu file</li> <li> <p>Verify HTTP access to the menu file</p> </li> <li> <p>OpenShift options not working:</p> </li> <li>Verify ISOs exist at the specified locations</li> <li>Check permissions on ISO files</li> <li>Validate HTTP access to the ISOs</li> </ol>"},{"location":"PROJECT_COMPLETION/","title":"OpenShift Multiboot System - Project Completion Plan","text":""},{"location":"PROJECT_COMPLETION/#project-status-overview","title":"Project Status Overview","text":"<p>The OpenShift Multiboot System project has made significant progress with several key components implemented:</p> <ul> <li>\u2705 Port configuration fixed (TrueNAS connectivity)</li> <li>\u2705 Netboot.xyz integration implemented</li> <li>\u2705 Documentation completed</li> <li>\u2705 Testing framework established</li> </ul>"},{"location":"PROJECT_COMPLETION/#final-project-deliverables","title":"Final Project Deliverables","text":"<p>To complete this project with a definitive ending, the following deliverables must be finalized:</p>"},{"location":"PROJECT_COMPLETION/#1-generate-and-verify-openshift-isos-1-week","title":"1. Generate and Verify OpenShift ISOs (1 week)","text":"<ul> <li>Task: Create OpenShift ISOs for versions 4.16, 4.17, and 4.18</li> <li>Acceptance Criteria:</li> <li>All ISOs are verified bootable via both ISO and Netboot methods</li> <li>ISOs are properly stored on TrueNAS with correct permissions</li> <li>Verification tests pass for all boot methods</li> </ul>"},{"location":"PROJECT_COMPLETION/#2-deploy-to-production-environment-1-week","title":"2. Deploy to Production Environment (1 week)","text":"<ul> <li>Task: Deploy the complete system to production R630 servers</li> <li>Acceptance Criteria:</li> <li>All scripts successfully executed on production servers</li> <li>Each boot method verified with at least one server</li> <li>System administrators trained on usage</li> <li>Documentation available in production environment</li> </ul>"},{"location":"PROJECT_COMPLETION/#3-create-final-ci-pipeline-2-days","title":"3. Create Final CI Pipeline (2 days)","text":"<ul> <li>Task: Implement basic CI/CD for system components</li> <li>Acceptance Criteria:</li> <li>Automated testing for boot method switching</li> <li>Linting for all Python scripts</li> <li>Version-controlled deployment pipeline</li> </ul>"},{"location":"PROJECT_COMPLETION/#4-final-system-documentation-2-days","title":"4. Final System Documentation (2 days)","text":"<ul> <li>Task: Finalize all documentation</li> <li>Acceptance Criteria:</li> <li>Usage guide with examples for all commands</li> <li>Troubleshooting section added</li> <li>Architectural diagrams included</li> <li>Administrator checklist for verification</li> </ul>"},{"location":"PROJECT_COMPLETION/#project-completion-checklist","title":"Project Completion Checklist","text":"<ul> <li>[ ] All ISOs generated and verified (in progress - running finalize_deployment.sh)</li> <li>[ ] All boot methods tested on hardware</li> <li>[x] CI/CD pipeline implemented (GitHub Actions workflow added for linting, testing, and documentation)</li> <li>[x] Documentation finalized and approved (MkDocs setup complete, troubleshooting guide added)</li> <li>[x] System administrator handoff completed (comprehensive handoff document created)</li> <li>[ ] Final demo conducted with stakeholders</li> </ul>"},{"location":"PROJECT_COMPLETION/#project-acceptance-statement","title":"Project Acceptance Statement","text":"<p>This project will be considered COMPLETE when:</p> <ol> <li>All items in the completion checklist have been verified</li> <li>A successful demonstration has been conducted showing:</li> <li>Switching between OpenShift versions via iSCSI</li> <li>Booting a fresh installation via ISO</li> <li>Using the netboot menu to select versions</li> <li>The project has been formally handed over to operations team</li> </ol>"},{"location":"PROJECT_COMPLETION/#future-enhancements-out-of-scope","title":"Future Enhancements (Out of Scope)","text":"<p>The following enhancements are documented but explicitly defined as future projects:</p> <ol> <li>Multiple Server Orchestration</li> <li>Batch operations for multiple servers</li> <li> <p>Configuration management system integration</p> </li> <li> <p>Unified Command Interface</p> </li> <li>Development of r630-manager.py</li> <li> <p>Interactive CLI experience</p> </li> <li> <p>Boot Disk Management</p> </li> <li>Automated disk snapshots</li> <li>Clone functionality</li> <li> <p>Backup/restore operations</p> </li> <li> <p>Advanced Monitoring</p> </li> <li>Integration with monitoring systems</li> <li>Alerting on boot failures</li> <li>Performance metrics</li> </ol>"},{"location":"PROJECT_COMPLETION/#final-deployment-steps","title":"Final Deployment Steps","text":"<ol> <li> <p>Execute ISO generation for all versions:    <pre><code>./scripts/generate_openshift_iso.py --version 4.16 --rendezvous-ip 192.168.2.230\n./scripts/generate_openshift_iso.py --version 4.17 --rendezvous-ip 192.168.2.230\n./scripts/generate_openshift_iso.py --version 4.18 --rendezvous-ip 192.168.2.230\n</code></pre></p> </li> <li> <p>Configure netboot menu:    <pre><code>./scripts/setup_netboot.py --truenas-ip 192.168.2.245\n</code></pre></p> </li> <li> <p>Verify all boot methods:    <pre><code># Test iSCSI boot\n./scripts/switch_openshift.py --server 192.168.2.230 --method iscsi --version 4.18 --check-only\n\n# Test ISO boot\n./scripts/switch_openshift.py --server 192.168.2.230 --method iso --version 4.18 --check-only\n\n# Test netboot\n./scripts/switch_openshift.py --server 192.168.2.230 --method netboot --check-only\n</code></pre></p> </li> <li> <p>Run final validation tests:    <pre><code>./scripts/test_setup.sh\n</code></pre></p> </li> </ol> <p>This document serves as the definitive guide to project completion. All stakeholders should refer to this document to track progress toward final delivery.</p>"},{"location":"TEST_PLAN/","title":"Test Plan for Multiboot System","text":"<p>This document outlines a structured approach to testing the multiboot system implementation. We'll test each component step by step to verify functionality for both OpenShift deployments and alternative OS options through netboot.xyz.</p>"},{"location":"TEST_PLAN/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to TrueNAS Scale server (192.168.2.245)</li> <li>Access to Dell R630 servers (192.168.2.230, 192.168.2.232)</li> <li>Admin credentials for TrueNAS Scale</li> <li>Red Hat account with pull secret for OpenShift</li> </ul>"},{"location":"TEST_PLAN/#1-truenas-authentication","title":"1. TrueNAS Authentication","text":""},{"location":"TEST_PLAN/#11-test-truenas_wrappersh-setup","title":"1.1 Test truenas_wrapper.sh Setup","text":"<pre><code># Run wrapper script for first-time setup\n./scripts/truenas_wrapper.sh\n</code></pre> <p>Expected result: - Script should prompt for TrueNAS host and authentication method - User should be guided through credentials setup - Configuration file should be created at ~/.config/truenas/auth.json</p>"},{"location":"TEST_PLAN/#12-test-authentication-with-api-key","title":"1.2 Test Authentication with API Key","text":"<p>After generating an API key in TrueNAS UI:</p> <pre><code># Create configuration with API key\nmkdir -p ~/.config/truenas\ncat &gt; ~/.config/truenas/auth.json &lt;&lt; EOF\n{\n  \"host\": \"192.168.2.245\",\n  \"api_key\": \"YOUR_ACTUAL_API_KEY\"\n}\nEOF\nchmod 600 ~/.config/truenas/auth.json\n\n# Test wrapper with discovery command\n./scripts/truenas_wrapper.sh autodiscovery --discover-only\n</code></pre> <p>Expected result: - Script should connect to TrueNAS successfully - Should display discovered pools, datasets, zvols, etc.</p>"},{"location":"TEST_PLAN/#2-truenas-configuration","title":"2. TrueNAS Configuration","text":""},{"location":"TEST_PLAN/#21-test-truenas-autodiscovery","title":"2.1 Test TrueNAS Autodiscovery","text":"<pre><code># Run autodiscovery in discover-only mode first\n./scripts/truenas_wrapper.sh autodiscovery --discover-only\n</code></pre> <p>Expected result: - Script should analyze existing TrueNAS configuration - Should show what changes would be made for OpenShift multiboot setup</p>"},{"location":"TEST_PLAN/#22-apply-truenas-configuration","title":"2.2 Apply TrueNAS Configuration","text":"<pre><code># Apply needed changes to TrueNAS\n./scripts/truenas_wrapper.sh autodiscovery --apply\n</code></pre> <p>Expected result: - Script should create necessary datasets, zvols, and iSCSI targets - Should report successful configuration application</p>"},{"location":"TEST_PLAN/#3-openshift-iso-generation","title":"3. OpenShift ISO Generation","text":""},{"location":"TEST_PLAN/#31-test-iso-generation-parameters","title":"3.1 Test ISO Generation Parameters","text":"<pre><code># Check help information\n./scripts/generate_openshift_iso.py --help\n</code></pre> <p>Expected result: - Script should display usage information and required parameters</p>"},{"location":"TEST_PLAN/#32-generate-openshift-iso-dry-run","title":"3.2 Generate OpenShift ISO (Dry Run)","text":"<pre><code># Test with skip-upload to verify parameters without actually generating\n./scripts/generate_openshift_iso.py --version 4.18 --rendezvous-ip 192.168.2.230 --skip-upload --output-dir ./test_output\n</code></pre> <p>Expected result: - Script should validate parameters - Should create test_output directory with configuration files - Should not perform the actual ISO generation (depends on pull secret)</p>"},{"location":"TEST_PLAN/#4-openshift-boot-switching","title":"4. OpenShift Boot Switching","text":""},{"location":"TEST_PLAN/#41-test-iscsi-target-check","title":"4.1 Test iSCSI Target Check","text":"<pre><code># Check if OpenShift 4.18 iSCSI target is properly configured\n./scripts/switch_openshift.py --server 192.168.2.230 --method iscsi --version 4.18 --check-only\n</code></pre> <p>Expected result: - Script should check if the target exists in configuration - Should display target information if available</p>"},{"location":"TEST_PLAN/#42-test-iso-boot-check","title":"4.2 Test ISO Boot Check","text":"<pre><code># Check if OpenShift 4.18 ISO is available\n./scripts/switch_openshift.py --server 192.168.2.230 --method iso --version 4.18 --check-only\n</code></pre> <p>Expected result: - Script should check if the ISO is accessible - Should display ISO status</p>"},{"location":"TEST_PLAN/#43-test-netbootxyz-boot-configuration","title":"4.3 Test netboot.xyz Boot Configuration","text":"<pre><code># Check if netboot.xyz configuration is available\n./scripts/switch_openshift.py --server 192.168.2.230 --method netboot --check-only\n</code></pre> <p>Expected result: - Script should verify netboot.xyz configuration - Should display netboot configuration status</p>"},{"location":"TEST_PLAN/#44-test-multi-server-operations","title":"4.4 Test Multi-Server Operations","text":"<pre><code># Check configuration on both servers\nfor server in 192.168.2.230 192.168.2.232; do\n  ./scripts/switch_openshift.py --server $server --method iscsi --version 4.18 --check-only\ndone\n</code></pre> <p>Expected result: - Script should check both servers - Should display configuration status for each server</p>"},{"location":"TEST_PLAN/#5-end-to-end-test-optional","title":"5. End-to-End Test (Optional)","text":"<p>If time and resources permit:</p>"},{"location":"TEST_PLAN/#51-generate-actual-iso","title":"5.1 Generate Actual ISO","text":"<pre><code># Generate an actual OpenShift 4.18 ISO (requires pull secret)\n./scripts/generate_openshift_iso.py --version 4.18 --rendezvous-ip 192.168.2.230\n</code></pre>"},{"location":"TEST_PLAN/#52-configure-server-for-iso-boot","title":"5.2 Configure Server for ISO Boot","text":"<pre><code># Configure server to boot from ISO (without actual reboot)\n./scripts/switch_openshift.py --server 192.168.2.230 --method iso --version 4.18\n</code></pre>"},{"location":"TEST_PLAN/#53-after-installation-future-test","title":"5.3 After Installation (Future Test)","text":"<p>After an OpenShift instance is installed:</p> <pre><code># Switch to iSCSI boot\n./scripts/switch_openshift.py --server 192.168.2.230 --method iscsi --version 4.18\n</code></pre>"},{"location":"TEST_PLAN/#test-results-documentation","title":"Test Results Documentation","text":"<p>Document test results with the following format:</p> Test # Description Status Notes 1.1 truenas_wrapper.sh Setup 1.2 API Key Authentication 2.1 TrueNAS Autodiscovery 2.2 TrueNAS Configuration 3.1 ISO Gen Parameters 3.2 ISO Generation 4.1 iSCSI Target Check 4.2 ISO Boot Check 4.3 netboot.xyz Config 4.4 Multi-Server Ops 5.1 Actual ISO Generation 5.2 ISO Boot Config 5.3 iSCSI Boot Switch 5.4 Netboot Boot Test"},{"location":"TEST_RESULTS/","title":"Dell PowerEdge R630 OpenShift Multiboot System - Test Results","text":"<p>This document contains the results of testing the functionality of the multiboot system components, along with a list of open points and enhancement tasks.</p>"},{"location":"TEST_RESULTS/#test-environment","title":"Test Environment","text":"<ul> <li>TrueNAS Scale server: 192.168.2.245</li> <li>Dell R630 servers: 192.168.2.230, 192.168.2.232</li> <li>Testing date: December 4, 2025</li> </ul>"},{"location":"TEST_RESULTS/#component-testing-results","title":"Component Testing Results","text":""},{"location":"TEST_RESULTS/#1-truenas-authentication-connectivity","title":"1. TrueNAS Authentication &amp; Connectivity","text":"Test Description Result Notes 1.1 TrueNAS wrapper setup N/A Will test later with API key 1.2 API key authentication N/A Will test later with API key 1.3 Direct connection test \u2705 Success TrueNAS Scale server is accessible at https://192.168.2.245:444/api/v2.0"},{"location":"TEST_RESULTS/#2-storage-configuration","title":"2. Storage Configuration","text":"Test Description Result Notes 2.1 TrueNAS autodiscovery \u274c Failed Test script attempts to connect on port 443, but TrueNAS is running on port 444 2.2 ISCSI targets configuration \u2705 Success iSCSI targets are properly defined in config file 2.3 NFS shares for ISOs \u2753 Unknown Could not verify NFS shares without TrueNAS connection"},{"location":"TEST_RESULTS/#3-iso-generation-management","title":"3. ISO Generation &amp; Management","text":"Test Description Result Notes 3.1 ISO generation parameters \u2705 Success All required parameters are available: version, rendezvous-ip, domain, pull-secret 3.2 ISO generation (dry run) 3.3 ISO upload to TrueNAS"},{"location":"TEST_RESULTS/#4-boot-management","title":"4. Boot Management","text":"Test Description Result Notes 4.1 iSCSI target check \u2705 Success Target 'openshift_4_18' is available in configuration with correct IQN and IP 4.2 ISO availability check \u274c Failed ISO for OpenShift 4.18 is not accessible at the expected location 4.3 Boot method switching \u2705 Success Script supports both iSCSI and ISO boot methods with version selection"},{"location":"TEST_RESULTS/#open-points-enhancement-tasks","title":"Open Points &amp; Enhancement Tasks","text":"<p>Based on testing and code review, here are the identified open points and suggested enhancements:</p>"},{"location":"TEST_RESULTS/#critical-issues","title":"Critical Issues","text":"<ol> <li>Port configuration mismatch</li> <li>Status: Fixed \u2705</li> <li>Description: Scripts were using port 443, but the server is running on port 444</li> <li>Solution: Updated scripts to use the correct port (444) by default</li> <li> <p>Priority: High</p> </li> <li> <p>Missing ISOs</p> </li> <li>Status: Issue detected</li> <li>Description: The ISO check failed - OpenShift installation ISOs are not available at the expected location</li> <li>Solution: Generate and upload ISOs for each supported OpenShift version</li> <li>Priority: High</li> </ol>"},{"location":"TEST_RESULTS/#feature-enhancements","title":"Feature Enhancements","text":"<ol> <li>Complete netboot.xyz integration</li> <li>Status: Completed \u2705</li> <li>Description: Added netboot support using custom URL (https://netboot.omnisack.nl)</li> <li>Tasks:<ul> <li>\u2705 Added 'netboot' as a method option in switch_openshift.py</li> <li>\u2705 Implemented UEFI HTTP boot configuration </li> <li>\u2705 Created setup_netboot.py for custom OpenShift menu</li> </ul> </li> <li> <p>Priority: High</p> </li> <li> <p>Multiple server orchestration</p> </li> <li>Status: Partial</li> <li>Description: Current scripts support specifying servers individually, but lack batch operations</li> <li>Tasks:<ul> <li>Add capability to configure multiple servers simultaneously</li> <li>Implement parallel execution for faster deployment</li> <li>Create server group configuration files</li> </ul> </li> <li> <p>Priority: Medium</p> </li> <li> <p>Boot disk backup/restore</p> </li> <li>Status: Not implemented</li> <li>Description: No functionality to backup, snapshot or restore boot disks</li> <li>Tasks:<ul> <li>Implement ZFS snapshot creation for boot disks</li> <li>Add clone functionality for quick duplication</li> <li>Create snapshot scheduling and rotation</li> </ul> </li> <li> <p>Priority: Medium</p> </li> <li> <p>Unified command interface</p> </li> <li>Status: Not implemented</li> <li>Description: Currently using multiple scripts with different parameters</li> <li>Tasks:<ul> <li>Create a unified CLI tool with subcommands</li> <li>Standardize parameter naming across all operations</li> <li>Add interactive mode with guided prompts</li> </ul> </li> <li>Priority: Medium</li> </ol>"},{"location":"TEST_RESULTS/#code-improvements","title":"Code Improvements","text":"<ol> <li>Consistent connection parameters</li> <li>Status: Fixed \u2705</li> <li>Description: Scripts were using different methods to connect to TrueNAS </li> <li>Solution: Standardized connection handling to use port 444 across all scripts</li> <li> <p>Priority: High</p> </li> <li> <p>Refactor command execution</p> </li> <li>Status: Could be improved</li> <li>Description: Many scripts use direct subprocess calls to other Python scripts</li> <li>Solution: Refactor into proper Python modules with shared core functionality</li> <li> <p>Priority: Medium</p> </li> <li> <p>Improve error handling</p> </li> <li>Status: Basic</li> <li>Description: Current error handling is minimal, especially for network and API failures</li> <li>Solution: Implement comprehensive error handling with specific error types and recovery strategies</li> <li> <p>Priority: Medium</p> </li> <li> <p>Implement logging framework</p> <ul> <li>Status: Not implemented</li> <li>Description: Currently using print statements for output</li> <li>Solution: Replace with proper Python logging framework with configurable levels</li> <li>Priority: Medium</li> </ul> </li> </ol>"},{"location":"TEST_RESULTS/#documentation-testing","title":"Documentation &amp; Testing","text":"<ol> <li> <p>Create network requirements document</p> <ul> <li>Status: Not available</li> <li>Description: Missing documentation on network requirements</li> <li>Tasks:</li> <li>Document all required ports and protocols</li> <li>Create network topology diagram</li> <li>Add firewall configuration guidance</li> <li>Priority: Low</li> </ul> </li> <li> <p>Add automated testing</p> <ul> <li>Status: Basic test scripts available</li> <li>Description: Test coverage is limited to manual tests</li> <li>Tasks:</li> <li>Create unit tests for core functionality</li> <li>Implement integration tests for key workflows</li> <li>Add CI pipeline for automated testing</li> <li>Priority: Low</li> </ul> </li> </ol>"},{"location":"TEST_RESULTS/#next-steps","title":"Next Steps","text":"<ol> <li> <p>Generate and upload OpenShift ISOs for testing:    <pre><code>./scripts/generate_openshift_iso.py --version 4.18 --rendezvous-ip 192.168.2.230\n</code></pre></p> </li> <li> <p>Test the complete netboot functionality:    <pre><code>./scripts/setup_netboot.py --truenas-ip 192.168.2.245\n./scripts/switch_openshift.py --server 192.168.2.230 --method netboot --netboot-menu openshift --check-only\n</code></pre></p> </li> <li> <p>Implement the next round of enhancements (in order of priority):</p> </li> <li>Multiple server orchestration</li> <li>Boot disk backup/restore functionality</li> <li>Unified command interface (r630-manager.py)</li> </ol>"},{"location":"docs/ADMIN_HANDOFF/","title":"Admin Handoff","text":""},{"location":"docs/ADMIN_HANDOFF/#openshift-multiboot-system-administrator-handoff","title":"OpenShift Multiboot System - Administrator Handoff","text":"<p>This document serves as a comprehensive handoff guide for system administrators who will be managing the OpenShift Multiboot System. It covers the system architecture, maintenance procedures, and operational responsibilities.</p>"},{"location":"docs/ADMIN_HANDOFF/#system-overview","title":"System Overview","text":"<p>The OpenShift Multiboot System provides flexible boot options for Dell PowerEdge R630 servers, allowing administrators to: - Deploy multiple OpenShift versions on the same hardware - Switch between versions without reinstallation - Boot via iSCSI, ISO, or Netboot methods - Manage the entire workflow through a set of Python scripts</p>"},{"location":"docs/ADMIN_HANDOFF/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Dell R630 Servers \u2502     \u2502         TrueNAS Scale Server         \u2502\n\u2502  192.168.2.230    \u2502\u25c4\u2500\u2500\u2500\u2500\u2524             192.168.2.245            \u2502\n\u2502  192.168.2.232    \u2502     \u2502                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n         \u2502                \u2502 \u2502 iSCSI      \u2502  \u2502 OpenShift ISOs  \u2502  \u2502\n         \u2502                \u2502 \u2502 Boot       \u2502  \u2502 (Agent-based)   \u2502  \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 \u2502 Targets    \u2502  \u2502                 \u2502  \u2502\n                          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"docs/ADMIN_HANDOFF/#key-components","title":"Key Components","text":"<ol> <li>TrueNAS Scale (192.168.2.245)</li> <li>Storage for OpenShift ISOs and boot volumes</li> <li>iSCSI target provider</li> <li> <p>HTTP server for netboot menu</p> </li> <li> <p>Dell PowerEdge R630 Servers</p> </li> <li>Target servers to boot OpenShift</li> <li> <p>Configurable via iDRAC Redfish API</p> </li> <li> <p>OpenShift Versions</p> </li> <li>Multiple OpenShift versions (4.16, 4.17, 4.18)</li> <li>Boot volumes stored as zvols on TrueNAS</li> <li>ISOs stored in datasets on TrueNAS</li> </ol>"},{"location":"docs/ADMIN_HANDOFF/#credentials-and-access-management","title":"Credentials and Access Management","text":""},{"location":"docs/ADMIN_HANDOFF/#truenas-access","title":"TrueNAS Access","text":"<p>TrueNAS authentication can be configured in several ways (see TRUENAS_AUTHENTICATION.md for details):</p> <ol> <li>API Key (recommended for automation)</li> <li>Created in TrueNAS UI under Credentials \u2192 API Keys</li> <li>Stored locally in <code>~/.config/truenas/auth.json</code></li> <li> <p>Managed via the <code>truenas_wrapper.sh</code> script</p> </li> <li> <p>Username/Password</p> </li> <li>Alternative method, not recommended for production</li> <li>Can be used as a fallback if API keys are unavailable</li> </ol>"},{"location":"docs/ADMIN_HANDOFF/#idrac-access","title":"iDRAC Access","text":"<p>Dell iDRAC credentials are used for server management:</p> <ol> <li>Default Configuration</li> <li>Default credentials are used as fallbacks (root/calvin)</li> <li> <p>Production deployments should set these via environment variables:      <pre><code># EXAMPLE - Replace with your actual credentials\nexport IDRAC_USER=\"EXAMPLE_USERNAME_HERE\"\nexport IDRAC_PASSWORD=\"EXAMPLE_PASSWORD_HERE\"\n</code></pre></p> </li> <li> <p>Security Recommendations</p> </li> <li>Change default iDRAC passwords on all servers</li> <li>Use API keys or SSH keys where possible</li> <li>Restrict network access to management interfaces</li> </ol>"},{"location":"docs/ADMIN_HANDOFF/#regular-maintenance-tasks","title":"Regular Maintenance Tasks","text":""},{"location":"docs/ADMIN_HANDOFF/#weekly-tasks","title":"Weekly Tasks","text":"<ol> <li> <p>Verify iSCSI target availability <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method iscsi --version 4.18 --check-only\n</code></pre></p> </li> <li> <p>Verify ISOs are accessible <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method iso --version 4.18 --check-only\n</code></pre></p> </li> <li> <p>Check netboot service <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method netboot --check-only\n</code></pre></p> </li> </ol>"},{"location":"docs/ADMIN_HANDOFF/#monthly-tasks","title":"Monthly Tasks","text":"<ol> <li> <p>Update OpenShift ISOs for latest point releases <pre><code>./scripts/generate_openshift_iso.py --version 4.18 --rendezvous-ip 192.168.2.230\n</code></pre></p> </li> <li> <p>TrueNAS maintenance</p> </li> <li>Check disk health in TrueNAS UI</li> <li>Review storage pool usage and expand if necessary</li> <li> <p>Apply TrueNAS updates during maintenance windows</p> </li> <li> <p>Create ZFS snapshots of OpenShift volumes</p> </li> <li>Create snapshots in TrueNAS UI under Datasets</li> <li>Label with date and purpose</li> <li>Consider automating via TrueNAS API</li> </ol>"},{"location":"docs/ADMIN_HANDOFF/#quarterly-tasks","title":"Quarterly Tasks","text":"<ol> <li>Rotate API keys</li> <li>Generate new API keys in TrueNAS</li> <li>Update <code>~/.config/truenas/auth.json</code> with new keys</li> <li> <p>Test connectivity after rotation</p> </li> <li> <p>Review and update documentation</p> </li> <li>Verify all procedures are current</li> <li>Update IP addresses, URLs, or other configuration if changed</li> <li>Document any new issues in the troubleshooting guide</li> </ol>"},{"location":"docs/ADMIN_HANDOFF/#common-operations","title":"Common Operations","text":""},{"location":"docs/ADMIN_HANDOFF/#switching-openshift-versions","title":"Switching OpenShift Versions","text":"<p>To switch a server from one OpenShift version to another:</p> <pre><code># Switch to OpenShift 4.18\n./scripts/switch_openshift.py --server 192.168.2.230 --method iscsi --version 4.18 --reboot\n</code></pre>"},{"location":"docs/ADMIN_HANDOFF/#installing-a-fresh-openshift-instance","title":"Installing a Fresh OpenShift Instance","text":"<p>To install a fresh copy of OpenShift:</p> <pre><code># First generate/update the ISO\n./scripts/generate_openshift_iso.py --version 4.18 --rendezvous-ip 192.168.2.230\n\n# Configure for ISO boot\n./scripts/switch_openshift.py --server 192.168.2.230 --method iso --version 4.18 --reboot\n</code></pre>"},{"location":"docs/ADMIN_HANDOFF/#using-netboot-for-advanced-options","title":"Using Netboot for Advanced Options","text":"<p>To use the netboot menu:</p> <pre><code># Configure netboot menu\n./scripts/setup_netboot.py --truenas-ip 192.168.2.245\n\n# Boot from netboot\n./scripts/switch_openshift.py --server 192.168.2.230 --method netboot --reboot\n</code></pre>"},{"location":"docs/ADMIN_HANDOFF/#troubleshooting","title":"Troubleshooting","text":"<p>See the detailed TROUBLESHOOTING.md document for guidance on resolving common issues.</p> <p>For urgent support or problems not covered in the documentation:</p> <ol> <li>Check log files in the project directory (<code>*.log</code>)</li> <li>Review iDRAC logs through the web interface</li> <li>Contact system administrators at <code>admin@example.com</code></li> </ol>"},{"location":"docs/ADMIN_HANDOFF/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"docs/ADMIN_HANDOFF/#truenas-failure","title":"TrueNAS Failure","text":"<p>If the TrueNAS server fails:</p> <ol> <li>Restore TrueNAS from backup</li> <li>Verify ZFS pool import was successful</li> <li>Ensure iSCSI service is running</li> <li>Test connectivity with:    <pre><code>./scripts/test_truenas_connection.py\n</code></pre></li> </ol>"},{"location":"docs/ADMIN_HANDOFF/#server-boot-failure","title":"Server Boot Failure","text":"<p>If a server fails to boot:</p> <ol> <li>Connect to iDRAC console to observe boot messages</li> <li>Use iDRAC virtual console to diagnose issues</li> <li>Try alternative boot methods:    <pre><code># Try alternative boot method\n./scripts/switch_openshift.py --server 192.168.2.230 --method netboot --reboot\n</code></pre></li> </ol>"},{"location":"docs/ADMIN_HANDOFF/#data-recovery","title":"Data Recovery","text":"<p>To recover OpenShift data:</p> <ol> <li>Create a new zvol in TrueNAS</li> <li>Restore from the most recent snapshot</li> <li>Create a new iSCSI target for the recovered volume</li> <li>Update <code>iscsi_targets.json</code> with the new target</li> <li>Boot from the recovered volume</li> </ol>"},{"location":"docs/ADMIN_HANDOFF/#security-considerations","title":"Security Considerations","text":"<ol> <li>Network Isolation</li> <li>Keep management networks (iDRAC, TrueNAS admin) separate from data networks</li> <li> <p>Use firewall rules to restrict access</p> </li> <li> <p>Authentication</p> </li> <li>Rotate API keys quarterly</li> <li>Use complex passwords for all accounts</li> <li> <p>Consider LDAP integration for centralized authentication</p> </li> <li> <p>Monitoring</p> </li> <li>Set up monitoring for TrueNAS health</li> <li>Configure alerts for boot failures</li> <li>Monitor disk space on TrueNAS</li> </ol>"},{"location":"docs/ADMIN_HANDOFF/#contact-information","title":"Contact Information","text":"<ul> <li>Primary System Administrator: admin@example.com</li> <li>TrueNAS Support: truenas-admin@example.com</li> <li>OpenShift Support: openshift-admin@example.com</li> </ul>"},{"location":"docs/ADMIN_HANDOFF/#appendices","title":"Appendices","text":""},{"location":"docs/ADMIN_HANDOFF/#environment-variables","title":"Environment Variables","text":"Variable Name Purpose Example Value IDRAC_USER Username for iDRAC access root IDRAC_PASSWORD Password for iDRAC access ** TRUENAS_API_KEY API key for TrueNAS access 1-a2b3c4d5..."},{"location":"docs/ADMIN_HANDOFF/#useful-commands","title":"Useful Commands","text":"Command Purpose <code>./scripts/truenas_wrapper.sh</code> Set up or update TrueNAS credentials <code>./scripts/test_setup.sh</code> Verify entire system configuration <code>./scripts/finalize_deployment.sh</code> Run complete deployment process"},{"location":"docs/DEPLOYMENT_TRACKING/","title":"Deployment Tracking and Artifact Management","text":"<p>This document explains the deployment tracking system used for maintaining multiple R630 OpenShift installations with different configurations.</p>"},{"location":"docs/DEPLOYMENT_TRACKING/#deployment-identification","title":"Deployment Identification","text":"<p>Each deployment is uniquely identified using the following format: <pre><code>r630-{server_id}-{cluster_type}-{timestamp}\n</code></pre></p> <ul> <li>server_id: Numeric identifier for the physical server (e.g., 01, 02)</li> <li>cluster_type: Type of deployment (sno, ha, etc.)</li> <li>timestamp: YYYYMMDDHHMMSS format timestamp when deployment was initiated</li> </ul> <p>Example: <code>r630-01-sno-20250415103025</code></p>"},{"location":"docs/DEPLOYMENT_TRACKING/#system-overview","title":"System Overview","text":"<p>The deployment tracking system consists of several integrated components:</p> <ol> <li>Configuration Management:</li> <li>Server-specific configuration files in organized directories</li> <li>Timestamped file naming for version tracking</li> <li> <p>Metadata embedded in configuration files</p> </li> <li> <p>Artifact Collection:</p> </li> <li>Automatic collection of logs, credentials, and metadata</li> <li>Secure storage on TrueNAS Scale</li> <li> <p>Standardized directory structure across servers</p> </li> <li> <p>Script Integration:</p> </li> <li>Command-line flags for specifying server IDs</li> <li>Automatic artifact uploading at deployment completion</li> <li>GitHub Actions workflow integration</li> </ol>"},{"location":"docs/DEPLOYMENT_TRACKING/#configuration-storage","title":"Configuration Storage","text":"<p>Deployment configuration files are stored in: <pre><code>config/deployments/{server-id}/{deployment-id}.yaml\n</code></pre></p> <p>For example: <pre><code>config/deployments/r630-01/r630-01-sno-20250412160745.yaml\n</code></pre></p>"},{"location":"docs/DEPLOYMENT_TRACKING/#artifact-storage-on-truenas","title":"Artifact Storage on TrueNAS","text":"<p>For each deployment, artifacts are automatically collected and stored on TrueNAS Scale: <pre><code>/mnt/tank/deployment_artifacts/{server-id}/{deployment-id}/\n</code></pre></p> <p>The artifacts directory structure includes: <pre><code>r630-01/\n\u2514\u2500\u2500 r630-01-sno-20250412160745/\n    \u251c\u2500\u2500 logs/\n    \u2502   \u2514\u2500\u2500 deployment_r630-01-sno-20250412160745.log\n    \u251c\u2500\u2500 auth/\n    \u2502   \u2514\u2500\u2500 kubeconfig\n    \u2514\u2500\u2500 metadata/\n        \u2514\u2500\u2500 metadata.json\n</code></pre></p> <p>Artifacts include: - logs/: All installation logs - auth/: Kubeconfig and authentication information - metadata.json: Detailed information about the deployment</p>"},{"location":"docs/DEPLOYMENT_TRACKING/#security-considerations","title":"Security Considerations","text":""},{"location":"docs/DEPLOYMENT_TRACKING/#private-vs-public-information","title":"Private vs. Public Information","text":"<p>This deployment system maintains a clear separation between:</p> <ol> <li>Private Information (stored only on TrueNAS Scale):</li> <li>OpenShift pull secrets</li> <li>SSH keys</li> <li>Kubeadmin passwords</li> <li>Full, unredacted logs</li> <li> <p>Kubeconfig files with connection details</p> </li> <li> <p>Public Information (may appear in GitHub):</p> </li> <li>Deployment structure and scripts</li> <li>Configuration templates (without credentials)</li> <li>Sanitized examples and documentation</li> <li>Workflow definitions (using GitHub Secrets for credentials)</li> </ol>"},{"location":"docs/DEPLOYMENT_TRACKING/#safe-usage-guidelines","title":"Safe Usage Guidelines","text":"<p>When working with this system:</p> <ol> <li>Never commit files containing:</li> <li>TrueNAS credentials</li> <li>OpenShift pull secrets</li> <li>SSH private keys</li> <li>Kubeadmin passwords</li> <li> <p>Kubeconfig files</p> </li> <li> <p>Always use GitHub Secrets for:</p> </li> <li>API tokens</li> <li>Authentication credentials</li> <li>Pull secrets</li> <li> <p>Any sensitive environment variables</p> </li> <li> <p>Review logs and outputs before publishing examples to ensure they don't contain:</p> </li> <li>Internal IP addresses</li> <li>Hostnames</li> <li>Authentication tokens</li> <li>Session cookies</li> </ol>"},{"location":"docs/DEPLOYMENT_TRACKING/#using-the-system","title":"Using the System","text":""},{"location":"docs/DEPLOYMENT_TRACKING/#creating-a-new-deployment","title":"Creating a New Deployment","text":"<ol> <li> <p>Generate a values file with server identifier:    <pre><code>./scripts/generate_openshift_values.py \\\n  --node-ip 192.168.2.230 \\\n  --cluster-name sno \\\n  --server-id 01 \\\n  --base-domain intranet.lab\n</code></pre></p> </li> <li> <p>Generate and upload the ISO with artifact collection:    <pre><code>./scripts/generate_openshift_iso.py \\\n  --version 4.18 \\\n  --values-file config/deployments/r630-01/r630-01-sno-20250415103025.yaml\n</code></pre></p> </li> <li> <p>Run the deployment:    <pre><code>./scripts/finalize_deployment.sh \\\n  --server-id 01 \\\n  --deployment-name sno \\\n  --values-file config/deployments/r630-01/r630-01-sno-20250415103025.yaml\n</code></pre></p> </li> </ol>"},{"location":"docs/DEPLOYMENT_TRACKING/#testing-the-system","title":"Testing the System","text":"<p>A test script is available to verify the deployment tracking functionality without performing an actual deployment:</p> <pre><code>./scripts/test_deployment_tracking.sh\n</code></pre> <p>This script: - Creates mock configuration files with server IDs - Generates sample deployment artifacts (logs, kubeconfig) - Tests metadata collection and artifact organization - Verifies the upload process in mock mode</p>"},{"location":"docs/DEPLOYMENT_TRACKING/#manual-artifact-upload","title":"Manual Artifact Upload","text":"<p>You can manually upload deployment artifacts using:</p> <pre><code>./scripts/upload_deployment_artifacts.sh \\\n  --server-id 01 \\\n  --deployment-name sno \\\n  --log-file path/to/deployment.log \\\n  --kubeconfig path/to/kubeconfig \\\n  --metadata version=4.18 \\\n  --metadata status=COMPLETE\n</code></pre> <p>Options include: - <code>--server-id</code>: Server identifier (required) - <code>--deployment-name</code>: Type of deployment (default: sno) - <code>--timestamp</code>: Custom timestamp (default: current time) - <code>--log-file</code>: Path to deployment log - <code>--kubeconfig</code>: Path to kubeconfig file - <code>--metadata</code>: Add custom metadata (can be specified multiple times) - <code>--mock-mode</code>: Test mode - don't connect to TrueNAS</p>"},{"location":"docs/DEPLOYMENT_TRACKING/#accessing-deployment-information","title":"Accessing Deployment Information","text":"<ol> <li>Via the web dashboard on TrueNAS: http://192.168.2.245/deployments/</li> <li>Directly on TrueNAS: <code>/mnt/tank/deployment_artifacts/{server-id}/{deployment-id}/</code></li> </ol>"},{"location":"docs/DEPLOYMENT_TRACKING/#managing-multiple-servers","title":"Managing Multiple Servers","text":"<p>Each physical server gets its own ID and separate configuration paths, allowing for parallel deployments and distinct configuration tracking:</p> <ul> <li>Server-01: <code>r630-01/</code></li> <li>Configurations: <code>config/deployments/r630-01/</code></li> <li> <p>Artifacts: <code>/mnt/tank/deployment_artifacts/r630-01/</code></p> </li> <li> <p>Server-02: <code>r630-02/</code></p> </li> <li>Configurations: <code>config/deployments/r630-02/</code></li> <li>Artifacts: <code>/mnt/tank/deployment_artifacts/r630-02/</code></li> </ul>"},{"location":"docs/DEPLOYMENT_TRACKING/#truenas-setup","title":"TrueNAS Setup","text":"<p>The TrueNAS environment is automatically configured with the appropriate directory structure:</p> <pre><code># Run on TrueNAS server\n./scripts/setup_truenas.sh\n</code></pre> <p>This will create: - Basic dataset structure for deployment artifacts - Server-specific directories for multiple R630 servers - Appropriate permissions for secure access</p>"},{"location":"docs/FINAL_DEMO_PLAN/","title":"OpenShift Multiboot System - Final Demonstration Plan","text":"<p>This document outlines the plan for the final demonstration of the OpenShift Multiboot System to stakeholders. The demonstration will showcase the key capabilities of the system and demonstrate that all project requirements have been met.</p>"},{"location":"docs/FINAL_DEMO_PLAN/#demonstration-overview","title":"Demonstration Overview","text":"<p>The final demonstration will be a comprehensive walkthrough of the OpenShift Multiboot System. It will demonstrate the three primary boot methods and show the ability to switch between different OpenShift versions on Dell PowerEdge R630 hardware.</p>"},{"location":"docs/FINAL_DEMO_PLAN/#logistics","title":"Logistics","text":"<ul> <li>Date: TBD (To be scheduled after ISO generation completes)</li> <li>Duration: 60 minutes</li> <li>Location: Server Room / Remote video conference</li> <li>Presenters: System Administrator, Project Lead</li> <li>Audience: Operations Team, IT Management, OpenShift Administrators</li> </ul>"},{"location":"docs/FINAL_DEMO_PLAN/#required-resources","title":"Required Resources","text":"<ul> <li>Dell PowerEdge R630 servers (192.168.2.230, 192.168.2.232)</li> <li>TrueNAS Scale server (192.168.2.245)</li> <li>Network access to all servers</li> <li>Projection system or screen sharing for remote participants</li> <li>Access to iDRAC consoles for demonstration</li> </ul>"},{"location":"docs/FINAL_DEMO_PLAN/#demonstration-agenda","title":"Demonstration Agenda","text":""},{"location":"docs/FINAL_DEMO_PLAN/#1-introduction-5-minutes","title":"1. Introduction (5 minutes)","text":"<ul> <li>Project overview and goals</li> <li>System architecture explanation</li> <li>Key components and their roles</li> </ul>"},{"location":"docs/FINAL_DEMO_PLAN/#2-iscsi-boot-demonstration-15-minutes","title":"2. iSCSI Boot Demonstration (15 minutes)","text":"<ul> <li>Explain iSCSI boot process and configuration</li> <li>Show iSCSI targets on TrueNAS</li> <li>Demonstrate switching between OpenShift versions:   <pre><code># Switch to OpenShift 4.18\n./scripts/switch_openshift.py --server 192.168.2.230 --method iscsi --version 4.18 --reboot\n\n# Once booted, show OpenShift version\n\n# Switch to OpenShift 4.17\n./scripts/switch_openshift.py --server 192.168.2.230 --method iscsi --version 4.17 --reboot\n</code></pre></li> <li>Verify boot success via iDRAC console</li> <li>Show OpenShift web console access</li> </ul>"},{"location":"docs/FINAL_DEMO_PLAN/#3-iso-boot-demonstration-15-minutes","title":"3. ISO Boot Demonstration (15 minutes)","text":"<ul> <li>Explain agent-based installation process</li> <li>Show ISO generation process (or pre-generated ISO)   <pre><code>./scripts/generate_openshift_iso.py --version 4.18 --rendezvous-ip 192.168.2.232\n</code></pre></li> <li>Configure server for ISO boot   <pre><code>./scripts/switch_openshift.py --server 192.168.2.232 --method iso --version 4.18 --reboot\n</code></pre></li> <li>Show boot process via iDRAC console</li> <li>Explain installation process and customization options</li> </ul>"},{"location":"docs/FINAL_DEMO_PLAN/#4-netboot-demonstration-15-minutes","title":"4. Netboot Demonstration (15 minutes)","text":"<ul> <li>Explain netboot capabilities and configuration</li> <li>Show custom menu configuration   <pre><code>./scripts/setup_netboot.py --truenas-ip 192.168.2.245\n</code></pre></li> <li>Configure server for netboot   <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method netboot --reboot\n</code></pre></li> <li>Demonstrate netboot menu and selection process via iDRAC console</li> <li>Show how to select different OpenShift versions from the menu</li> </ul>"},{"location":"docs/FINAL_DEMO_PLAN/#5-documentation-and-administration-5-minutes","title":"5. Documentation and Administration (5 minutes)","text":"<ul> <li>Overview of documentation structure</li> <li>Highlight key administration procedures</li> <li>Show CI/CD pipeline in GitHub Actions</li> <li>Explain monitoring and maintenance procedures</li> </ul>"},{"location":"docs/FINAL_DEMO_PLAN/#6-qa-and-handoff-5-minutes","title":"6. Q&amp;A and Handoff (5 minutes)","text":"<ul> <li>Answer questions from stakeholders</li> <li>Formal handoff to operations team</li> <li>Discussion of future enhancements</li> </ul>"},{"location":"docs/FINAL_DEMO_PLAN/#success-criteria","title":"Success Criteria","text":"<p>The demonstration will be considered successful if:</p> <ol> <li>All three boot methods are successfully demonstrated</li> <li>Switching between OpenShift versions is shown to be working</li> <li>All stakeholder questions are answered satisfactorily</li> <li>Operations team confirms readiness to take ownership of the system</li> </ol>"},{"location":"docs/FINAL_DEMO_PLAN/#pre-demonstration-checklist","title":"Pre-Demonstration Checklist","text":"<ul> <li>[ ] Verify all ISOs are generated and accessible</li> <li>[ ] Test all boot methods on demonstration hardware</li> <li>[ ] Ensure network connectivity between all components</li> <li>[ ] Prepare iDRAC console access for demonstration</li> <li>[ ] Rehearse demonstration steps and timing</li> <li>[ ] Ensure all documentation is up to date and accessible</li> <li>[ ] Prepare handoff documentation for operations team</li> </ul>"},{"location":"docs/FINAL_DEMO_PLAN/#post-demonstration-actions","title":"Post-Demonstration Actions","text":"<ol> <li>Gather feedback from stakeholders</li> <li>Document any issues or enhancement requests</li> <li>Complete formal handoff to operations team</li> <li>Update PROJECT_COMPLETION.md to mark final demo as completed</li> <li>Close out project in project management system</li> </ol>"},{"location":"docs/FINAL_DEMO_PLAN/#contingency-plans","title":"Contingency Plans","text":""},{"location":"docs/FINAL_DEMO_PLAN/#network-issues","title":"Network Issues","text":"<ul> <li>Have screenshots of successful operations ready to show</li> <li>Prepare recorded video of boot processes as backup</li> </ul>"},{"location":"docs/FINAL_DEMO_PLAN/#hardware-issues","title":"Hardware Issues","text":"<ul> <li>Have multiple servers available for demonstration</li> <li>Be prepared to show pre-recorded demos if hardware is unavailable</li> </ul>"},{"location":"docs/FINAL_DEMO_PLAN/#time-management","title":"Time Management","text":"<ul> <li>Prioritize iSCSI and netboot demonstrations if time runs short</li> <li>Prepare abbreviated demo flow if needed</li> </ul> <p>This document serves as the plan for the final demonstration of the OpenShift Multiboot System. After successful completion of this demonstration, the project will be considered complete and ready for handoff to the operations team.</p>"},{"location":"docs/GITHUB_ACTIONS_USAGE/","title":"Using GitHub Actions for OpenShift ISO Generation","text":"<p>This document explains how to use GitHub Actions to generate OpenShift ISOs on x86_64 runners, which is particularly useful when developing on non-x86_64 machines (like Apple Silicon Macs).</p>"},{"location":"docs/GITHUB_ACTIONS_USAGE/#why-use-github-actions","title":"Why Use GitHub Actions?","text":"<p>The OpenShift installer is designed for x86_64 architectures, and running it on ARM-based systems (like Apple Silicon Macs) can be problematic and slow. By leveraging GitHub Actions:</p> <ol> <li>ISOs are generated on x86_64 GitHub-hosted runners</li> <li>The process is faster and more reliable</li> <li>We avoid architecture compatibility issues</li> <li>The generated ISOs can be uploaded directly to TrueNAS</li> </ol>"},{"location":"docs/GITHUB_ACTIONS_USAGE/#prerequisites","title":"Prerequisites","text":"<p>Before using the GitHub Actions workflow, ensure you have:</p> <ol> <li> <p>GitHub CLI Installed:    <pre><code># macOS (Homebrew)\nbrew install gh\n\n# Linux\nsudo apt install gh\n</code></pre></p> </li> <li> <p>GitHub CLI Authenticated:    <pre><code>gh auth login\n</code></pre></p> </li> <li> <p>Required GitHub Secrets Set Up:</p> </li> <li><code>OPENSHIFT_PULL_SECRET</code>: Your Red Hat OpenShift pull secret</li> <li><code>TRUENAS_SSH_KEY</code>: SSH private key for TrueNAS access</li> <li><code>TRUENAS_KNOWN_HOSTS</code>: Known hosts entry for TrueNAS server</li> </ol>"},{"location":"docs/GITHUB_ACTIONS_USAGE/#how-to-configure-github-secrets","title":"How to Configure GitHub Secrets","text":"<ol> <li>Go to your GitHub repository settings</li> <li>Navigate to \"Secrets and variables\" \u2192 \"Actions\"</li> <li> <p>Add the following repository secrets:</p> </li> <li> <p>OPENSHIFT_PULL_SECRET:</p> <ul> <li>Content: Your OpenShift pull secret (from https://console.redhat.com/openshift/install/pull-secret)</li> <li>Format: JSON string</li> </ul> </li> <li> <p>TRUENAS_SSH_KEY:</p> <ul> <li>Content: SSH private key for TrueNAS access</li> <li>To generate:    <pre><code>ssh-keygen -t ed25519 -f truenas_key -N \"\"\n# Then copy the public key to TrueNAS\nssh-copy-id -i truenas_key.pub root@192.168.2.245\n# Use the private key content for the secret\ncat truenas_key\n</code></pre></li> </ul> </li> <li> <p>TRUENAS_KNOWN_HOSTS:</p> <ul> <li>Content: SSH known hosts entry for TrueNAS</li> <li>To generate:    <pre><code>ssh-keyscan -H 192.168.2.245\n</code></pre></li> </ul> </li> </ol>"},{"location":"docs/GITHUB_ACTIONS_USAGE/#using-the-workflow","title":"Using the Workflow","text":""},{"location":"docs/GITHUB_ACTIONS_USAGE/#method-1-using-finalize_deploymentsh","title":"Method 1: Using finalize_deployment.sh","text":"<p>The simplest way to use the GitHub Actions workflow is through the updated <code>finalize_deployment.sh</code> script, which automatically:</p> <ol> <li>Triggers the workflow for each OpenShift version</li> <li>Monitors workflow progress</li> <li>Verifies the ISOs are available on TrueNAS</li> </ol> <pre><code>./scripts/finalize_deployment.sh\n</code></pre>"},{"location":"docs/GITHUB_ACTIONS_USAGE/#method-2-manually-triggering-the-workflow","title":"Method 2: Manually Triggering the Workflow","text":"<p>You can also manually trigger the workflow using the GitHub CLI:</p> <pre><code># Get your repository name\nREPO_NAME=$(git remote get-url origin | sed -n 's/.*github.com[:/]\\(.*\\).git/\\1/p')\n\n# Trigger the workflow for OpenShift 4.18\ngh workflow run generate_iso.yml -R \"$REPO_NAME\" \\\n  -f version=\"4.18\" \\\n  -f rendezvous_ip=\"192.168.2.230\" \\\n  -f truenas_ip=\"192.168.2.245\"\n</code></pre> <p>Or through the GitHub web interface:</p> <ol> <li>Go to the \"Actions\" tab in your repository</li> <li>Select \"Generate OpenShift ISOs\" workflow</li> <li>Click \"Run workflow\"</li> <li>Enter parameters:</li> <li>Version (e.g., 4.18)</li> <li>Rendezvous IP (e.g., 192.168.2.230)</li> <li>TrueNAS IP (e.g., 192.168.2.245)</li> <li>Skip upload (optional)</li> <li>Click \"Run workflow\"</li> </ol>"},{"location":"docs/GITHUB_ACTIONS_USAGE/#workflow-output","title":"Workflow Output","text":"<p>The workflow produces the following outputs:</p> <ol> <li>GitHub Artifacts: The ISO is uploaded as a GitHub artifact, available for 90 days</li> <li>TrueNAS Upload: The ISO is uploaded to TrueNAS at <code>/mnt/tank/openshift_isos/{version}/agent.x86_64.iso</code></li> <li>HTTP Access: The ISO is accessible via HTTP at <code>http://{truenas_ip}/openshift_isos/{version}/agent.x86_64.iso</code></li> </ol>"},{"location":"docs/GITHUB_ACTIONS_USAGE/#troubleshooting","title":"Troubleshooting","text":"<p>If the workflow fails, check:</p> <ol> <li>Workflow Logs: Examine the workflow logs in GitHub Actions for error messages</li> <li>Secret Values: Verify that all secrets are correctly set up</li> <li>TrueNAS Connectivity: Ensure the TrueNAS server is accessible from GitHub Actions runners</li> <li>SSH Key Permissions: Verify the SSH key has proper permissions on TrueNAS</li> </ol> <p>If you need to debug the workflow locally, you can use the following approach:</p> <pre><code># Run the script with the --skip-upload flag to test ISO generation without uploading\n./scripts/generate_openshift_iso.py --version 4.18 --rendezvous-ip 192.168.2.230 --skip-upload\n</code></pre>"},{"location":"docs/GITHUB_ACTIONS_USAGE/#conclusion","title":"Conclusion","text":"<p>Using GitHub Actions for OpenShift ISO generation provides a reliable, architecture-independent method to prepare ISOs for deployment on Dell PowerEdge R630 servers. This approach is especially valuable for developers using non-x86_64 machines like Apple Silicon Macs.</p>"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/","title":"iSCSI and OpenShift Integration Guide","text":"<p>This document describes how to use the integrated workflow for creating iSCSI targets and deploying OpenShift on Dell R630 servers. The integration streamlines the process by automatically coordinating iSCSI target creation, iSCSI boot configuration, and OpenShift agent-based installation.</p>"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#overview","title":"Overview","text":"<p>The integration process addresses several challenges:</p> <ol> <li>Device Path Consistency: Ensures that iSCSI targets are consistently mapped to expected device paths</li> <li>Configuration Coordination: Automatically generates matching iSCSI and OpenShift configurations</li> <li>Reduced Manual Steps: Integrates multiple scripts into a single workflow</li> </ol>"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#prerequisites","title":"Prerequisites","text":"<ul> <li>TrueNAS server (192.168.2.245) with iSCSI service enabled</li> <li>Dell R630 server with iDRAC access</li> <li>Network connectivity between all components</li> <li>OpenShift pull secret in <code>~/.openshift/pull-secret</code></li> </ul>"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#integration-script","title":"Integration Script","text":"<p>The <code>integrate_iscsi_openshift.py</code> script ties together iSCSI target creation, iSCSI boot configuration, and OpenShift ISO generation.</p>"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#basic-usage","title":"Basic Usage","text":"<pre><code>./scripts/integrate_iscsi_openshift.py \\\n  --server-id 01 \\\n  --hostname humpty \\\n  --node-ip 192.168.2.90 \\\n  --mac-address e4:43:4b:44:5b:10 \\\n  --base-domain omnisack.nl \\\n  --openshift-version stable\n</code></pre> <p>This command will:</p> <ol> <li>Create an iSCSI target on TrueNAS</li> <li>Configure iSCSI boot on the Dell R630 server via iDRAC</li> <li>Generate OpenShift configuration values with the correct device path</li> <li>Create an OpenShift agent-based installation ISO</li> </ol>"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#parameters","title":"Parameters","text":"Parameter Description Default <code>--server-id</code> Server ID (e.g., 01) Required <code>--hostname</code> Server hostname Required <code>--node-ip</code> Server IP address Required <code>--mac-address</code> Server MAC address for primary NIC Required <code>--base-domain</code> Base domain for the cluster example.com <code>--openshift-version</code> OpenShift version stable <code>--idrac-ip</code> iDRAC IP address 192.168.2.230 <code>--idrac-user</code> iDRAC username root <code>--idrac-password</code> iDRAC password calvin <code>--truenas-ip</code> TrueNAS IP address 192.168.2.245 <code>--truenas-user</code> TrueNAS username root <code>--device-path</code> Override iSCSI device path /dev/sda <code>--output-dir</code> Output directory for generated files ./test_run_[hostname]"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#selective-execution","title":"Selective Execution","text":"<p>The script supports skipping certain steps if needed:</p> <pre><code>./scripts/integrate_iscsi_openshift.py \\\n  --server-id 01 \\\n  --hostname humpty \\\n  --node-ip 192.168.2.90 \\\n  --mac-address e4:43:4b:44:5b:10 \\\n  --skip-target-creation \\\n  --skip-iscsi-config\n</code></pre> <p>Available skip options: - <code>--skip-target-creation</code>: Skip TrueNAS iSCSI target creation - <code>--skip-iscsi-config</code>: Skip iSCSI boot configuration - <code>--skip-iso-generation</code>: Skip OpenShift ISO generation</p>"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#iscsi-device-mapping","title":"iSCSI Device Mapping","text":"<p>The script maintains a mapping between iSCSI targets and device paths in <code>config/iscsi_device_mapping.json</code>. This ensures consistent device paths across reboots and reinstallations.</p> <p>Example mapping: <pre><code>{\n  \"targets\": {\n    \"iqn.2005-10.org.freenas.ctl:iscsi.r630-01.openshift4_18\": {\n      \"server_id\": \"01\",\n      \"hostname\": \"humpty\",\n      \"device_path\": \"/dev/sda\",\n      \"zvol_path\": \"/dev/zvol/tank/openshift_installations/r630_01_4_18\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#openshift-configuration-for-iscsi-boot","title":"OpenShift Configuration for iSCSI Boot","text":"<p>The script automatically generates the correct OpenShift agent-based installation configuration with the proper device paths:</p> <ol> <li> <p>In <code>install-config.yaml</code>:    <pre><code>bootstrapInPlace:\n  installationDisk: \"/dev/sda\"  # From device mapping\n</code></pre></p> </li> <li> <p>In <code>agent-config.yaml</code>:    <pre><code>rootDeviceHints:\n  deviceName: \"/dev/sda\"  # From device mapping\n</code></pre></p> </li> </ol>"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#device-path-issues","title":"Device Path Issues","text":"<p>If the iSCSI device doesn't appear at the expected path:</p> <ol> <li>Boot from the OpenShift ISO</li> <li>From the CoreOS live environment, check the actual device path:    <pre><code>lsblk -f\nls -l /dev/disk/by-path/*iscsi*\n</code></pre></li> <li>Re-run the integration script with the correct device path:    <pre><code>./scripts/integrate_iscsi_openshift.py --device-path /dev/sdX ...\n</code></pre></li> </ol>"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#iscsi-boot-issues","title":"iSCSI Boot Issues","text":"<p>If the server doesn't boot from iSCSI:</p> <ol> <li>Verify that iSCSI initiator is enabled in the BIOS</li> <li>Verify iSCSI target connectivity:    <pre><code>iscsiadm -m discovery -t sendtargets -p 192.168.2.245\n</code></pre></li> <li>Check target configuration on TrueNAS</li> </ol>"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#examples","title":"Examples","text":""},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#complete-setup-for-a-new-server","title":"Complete Setup for a New Server","text":"<pre><code># Create iSCSI target, configure iSCSI boot, and generate OpenShift ISO\n./scripts/integrate_iscsi_openshift.py \\\n  --server-id 01 \\\n  --hostname humpty \\\n  --node-ip 192.168.2.90 \\\n  --mac-address e4:43:4b:44:5b:10 \\\n  --base-domain omnisack.nl \\\n  --openshift-version stable\n</code></pre>"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#regenerate-iso-with-existing-iscsi-target","title":"Regenerate ISO with Existing iSCSI Target","text":"<pre><code># Skip target creation and iSCSI boot configuration\n./scripts/integrate_iscsi_openshift.py \\\n  --server-id 01 \\\n  --hostname humpty \\\n  --node-ip 192.168.2.90 \\\n  --mac-address e4:43:4b:44:5b:10 \\\n  --skip-target-creation \\\n  --skip-iscsi-config\n</code></pre>"},{"location":"docs/ISCSI_OPENSHIFT_INTEGRATION/#references","title":"References","text":"<ul> <li>OpenShift Agent-Based ISO Generation</li> <li>R630 iSCSI Boot Guide</li> <li>TrueNAS iSCSI Finding Block Devices with OpenShift Agent-Based Installer</li> </ul>"},{"location":"docs/OPENSHIFT_VALUES_SYSTEM/","title":"OpenShift Values System","text":"<p>This document explains the OpenShift values system that simplifies the configuration and deployment of OpenShift clusters, particularly Single-Node OpenShift (SNO) deployments.</p>"},{"location":"docs/OPENSHIFT_VALUES_SYSTEM/#overview","title":"Overview","text":"<p>The OpenShift values system provides a structured way to define and manage configuration parameters for OpenShift installations. It offers several advantages:</p> <ol> <li>Standardized Configuration: Consistent format for all OpenShift installations</li> <li>Reusability: Easily reuse configurations across multiple deployments</li> <li>Version Control: Track changes to configurations in Git</li> <li>CI/CD Integration: Use configurations in automated pipelines with GitHub Actions</li> <li>Simplified Updates: Central location to update parameters</li> </ol>"},{"location":"docs/OPENSHIFT_VALUES_SYSTEM/#values-file-format","title":"Values File Format","text":"<p>The values file follows the OpenShift install-config.yaml format with additional sections for SNO-specific configuration. It includes:</p> <ul> <li>Basic cluster information (name, domain)</li> <li>Networking configuration</li> <li>Single-node specifics (node IP, API VIP, Ingress VIP)</li> <li>Storage configuration</li> </ul> <p>Example:</p> <pre><code>apiVersion: v1\nbaseDomain: intranet.lab\ncompute:\n- architecture: amd64\n  hyperthreading: Enabled\n  name: worker\n  replicas: 0  # SNO has 0 worker nodes\ncontrolPlane:\n  architecture: amd64\n  hyperthreading: Enabled\n  name: master\n  replicas: 1  # SNO has 1 master node\nmetadata:\n  name: r630-sno\nnetworking:\n  clusterNetwork:\n  - cidr: 10.128.0.0/14\n    hostPrefix: 23\n  machineNetwork:\n  - cidr: 192.168.2.0/24\n  networkType: OVNKubernetes\n  serviceNetwork:\n  - 172.30.0.0/16\nplatform:\n  none: {}\npublish: External\nsno:\n  nodeIP: 192.168.2.230\n  apiVIP: 192.168.2.231\n  ingressVIP: 192.168.2.232\n  domain: apps.r630-sno.intranet.lab\n  hostname: sno\n</code></pre>"},{"location":"docs/OPENSHIFT_VALUES_SYSTEM/#generating-values-files","title":"Generating Values Files","text":"<p>The <code>generate_openshift_values.py</code> script helps create customized values files:</p> <pre><code>./scripts/generate_openshift_values.py \\\n  --node-ip 192.168.2.230 \\\n  --cluster-name r630-sno \\\n  --base-domain intranet.lab\n</code></pre>"},{"location":"docs/OPENSHIFT_VALUES_SYSTEM/#script-options","title":"Script Options","text":"<ul> <li><code>--cluster-name</code>: Name of the cluster (default: sno)</li> <li><code>--base-domain</code>: Base domain for the cluster (default: example.com)</li> <li><code>--hostname</code>: Hostname for the single node (default: sno)</li> <li><code>--node-ip</code>: IP address of the single node (required)</li> <li><code>--api-vip</code>: Virtual IP for the API server (default: node_ip + 1)</li> <li><code>--ingress-vip</code>: Virtual IP for the ingress controller (default: node_ip + 2)</li> <li><code>--output-dir</code>: Output directory (default: config directory)</li> </ul> <p>The script will generate a values file named <code>openshift_install_values_&lt;cluster-name&gt;.yaml</code> in the config directory.</p>"},{"location":"docs/OPENSHIFT_VALUES_SYSTEM/#using-values-files-with-iso-generation","title":"Using Values Files with ISO Generation","text":""},{"location":"docs/OPENSHIFT_VALUES_SYSTEM/#cli-usage","title":"CLI Usage","text":"<p>You can use a values file with the ISO generation script:</p> <pre><code>./scripts/generate_openshift_iso.py \\\n  --version 4.18 \\\n  --rendezvous-ip 192.168.2.230 \\\n  --values-file config/openshift_install_values_r630-sno.yaml\n</code></pre> <p>When a values file is provided, it takes precedence over command-line arguments for parameters like domain and network settings.</p>"},{"location":"docs/OPENSHIFT_VALUES_SYSTEM/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<p>The GitHub Actions workflow supports values files:</p> <pre><code>name: Generate ISO with Values File\n\non:\n  workflow_dispatch:\n    inputs:\n      version:\n        description: 'OpenShift version'\n        default: '4.18'\n      values_file:\n        description: 'Values file name (in config directory)'\n        default: 'openshift_install_values_r630-sno.yaml'\n</code></pre> <p>You can trigger the workflow from the GitHub UI or using the GitHub CLI:</p> <pre><code>gh workflow run generate_iso.yml \\\n  -f version=\"4.18\" \\\n  -f values_file=\"openshift_install_values_r630-sno.yaml\"\n</code></pre> <p>Or generate values and run the workflow in one command:</p> <pre><code># Generate values file\n./scripts/generate_openshift_values.py --node-ip 192.168.2.230 --cluster-name r630-sno\n\n# Trigger workflow with the generated values file\ngh workflow run generate_iso.yml \\\n  -f version=\"4.18\" \\\n  -f values_file=\"openshift_install_values_r630-sno.yaml\"\n</code></pre>"},{"location":"docs/OPENSHIFT_VALUES_SYSTEM/#values-file-locations","title":"Values File Locations","text":"<p>The scripts will look for values files in the following locations:</p> <ol> <li>Path specified in the <code>--values-file</code> argument</li> <li><code>config/openshift_install_values_&lt;cluster-name&gt;.yaml</code></li> </ol>"},{"location":"docs/OPENSHIFT_VALUES_SYSTEM/#advanced-usage-customizing-values-files","title":"Advanced Usage: Customizing Values Files","text":"<p>While the generator script creates standard values files, you can manually edit them to:</p> <ol> <li>Add custom network configurations</li> <li>Configure storage for the cluster</li> <li>Set up additional authentication methods</li> <li>Configure proxy settings</li> <li>Define custom manifests</li> </ol> <p>After editing, you can use the customized values file with the ISO generation process as described above.</p>"},{"location":"docs/OPENSHIFT_VALUES_SYSTEM/#integration-with-finalize_deploymentsh","title":"Integration with finalize_deployment.sh","text":"<p>The finalize_deployment.sh script has been updated to work with the values system:</p> <pre><code># Generate values file for each OpenShift version\n./scripts/generate_openshift_values.py --node-ip 192.168.2.230 --cluster-name r630-4.18\n\n# Run deployment with values file\n./scripts/finalize_deployment.sh --values-file openshift_install_values_r630-4.18.yaml\n</code></pre> <p>This ensures consistent configuration across different OpenShift versions and deployments.</p>"},{"location":"docs/SECRETS_PROVIDER/","title":"Secrets Provider System","text":""},{"location":"docs/SECRETS_PROVIDER/#secrets-provider-system","title":"Secrets Provider System","text":"<p>This document explains the secrets provider system that has been implemented to securely manage sensitive information in the OpenShift deployment process.</p>"},{"location":"docs/SECRETS_PROVIDER/#overview","title":"Overview","text":"<p>The secrets provider system is an abstraction layer that handles secrets management through different backends. It allows the system to:</p> <ol> <li>Securely store secrets in different storage backends</li> <li>Reference secrets in configuration files without including their actual values</li> <li>Isolate sensitive information from the codebase and configuration repositories</li> <li>Provide a migration path to more robust secrets management solutions like HashiCorp Vault</li> </ol>"},{"location":"docs/SECRETS_PROVIDER/#architecture","title":"Architecture","text":"<p>The secrets provider is designed as an abstracted interface with pluggable backends:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Configuration   \u2502     \u2502 Secrets Provider  \u2502     \u2502 Storage Backends   \u2502\n\u2502 with References \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Interface        \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 (File/TrueNAS/Vault)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"docs/SECRETS_PROVIDER/#current-backends","title":"Current Backends","text":"<ol> <li>File Backend (Default): Stores secrets in the local filesystem</li> <li>Path: <code>~/.openshift/secrets/</code></li> <li> <p>Simple and easy to use for development</p> </li> <li> <p>TrueNAS Backend: Stores secrets on a TrueNAS server</p> </li> <li>Path: <code>/mnt/tank/openshift_secrets/</code></li> <li> <p>Good for persistent storage in the lab environment</p> </li> <li> <p>Vault Backend (Planned): Will store secrets in HashiCorp Vault</p> </li> <li>Provides enterprise-grade security features</li> <li>Supports access control, auditing, and versioning</li> </ol>"},{"location":"docs/SECRETS_PROVIDER/#using-secret-references","title":"Using Secret References","text":"<p>Configuration files can reference secrets using a special syntax:</p> <pre><code># Example OpenShift values file with secret references\napiVersion: v1\nbaseDomain: example.com\n# ...other configuration...\n\n# Reference secrets that will be injected\nsecretReferences:\n  pullSecret: ${secret:openshift/pull-secret}\n  sshKey: ${secret:openshift/ssh-key}\n</code></pre> <p>The format for secret references is: - <code>${secret:path/to/secret}</code> - Simple secret reference - <code>${secret:path/to/secret:key}</code> - Reference a key in a structured secret (JSON)</p>"},{"location":"docs/SECRETS_PROVIDER/#cli-interface","title":"CLI Interface","text":"<p>The secrets provider comes with a command-line interface for managing secrets:</p> <pre><code># Get a secret\n./scripts/secrets_provider.py get openshift/pull-secret\n\n# Store a secret\n./scripts/secrets_provider.py put openshift/ssh-key --file ~/.ssh/id_rsa.pub\n\n# Check or set the provider type\n./scripts/secrets_provider.py provider\n./scripts/secrets_provider.py provider --type truenas\n</code></pre>"},{"location":"docs/SECRETS_PROVIDER/#environment-variables","title":"Environment Variables","text":"<p>The system can be configured using environment variables:</p> Variable Description Default <code>SECRETS_PROVIDER</code> The backend provider to use <code>file</code> <code>LOCAL_SECRETS_DIR</code> Directory for file-based secrets <code>~/.openshift/secrets</code> <code>TRUENAS_IP</code> IP address of TrueNAS server <code>192.168.2.245</code> <code>TRUENAS_USER</code> Username for TrueNAS SSH access <code>root</code> <code>TRUENAS_SECRETS_PATH</code> Path on TrueNAS for secrets <code>/mnt/tank/openshift_secrets</code>"},{"location":"docs/SECRETS_PROVIDER/#integration-with-github-actions","title":"Integration with GitHub Actions","text":"<p>The GitHub Actions workflow integrates with the secrets provider:</p> <ol> <li>It sets up the secrets provider with GitHub Secrets as inputs</li> <li>Stores OpenShift pull secret and SSH key in the local provider</li> <li>Process any secret references in the values file</li> <li>Backs up configurations to TrueNAS (with sensitive data sanitized)</li> <li>Cleans up sensitive files after the workflow completes</li> </ol>"},{"location":"docs/SECRETS_PROVIDER/#migration-to-hashicorp-vault","title":"Migration to HashiCorp Vault","text":"<p>To migrate to HashiCorp Vault in the future:</p> <ol> <li>Set up a Vault server and configure access</li> <li>Import existing secrets into Vault</li> <li>Implement the Vault backend in the secrets provider</li> <li>Update environment variables to use Vault instead of file/TrueNAS</li> </ol> <p>No changes to configuration files or workflows would be needed, as they use the abstraction layer through secret references.</p>"},{"location":"docs/SECRETS_PROVIDER/#security-considerations","title":"Security Considerations","text":"<ul> <li>Never commit files containing secrets to Git</li> <li>The <code>.gitignore</code> file excludes patterns like <code>*secret*</code> and <code>*credential*</code></li> <li>When using the TrueNAS backend, access controls should be configured on the TrueNAS server</li> <li>The system includes sanitization for logs and artifacts to prevent accidental exposure</li> <li>The GitHub Actions workflow includes cleanup steps to remove sensitive files</li> </ul>"},{"location":"docs/TROUBLESHOOTING/","title":"OpenShift Multiboot System - Troubleshooting Guide","text":"<p>This document provides solutions for common issues that might be encountered when using the OpenShift Multiboot System. </p>"},{"location":"docs/TROUBLESHOOTING/#table-of-contents","title":"Table of Contents","text":"<ul> <li>TrueNAS Connectivity Issues</li> <li>iSCSI Boot Problems</li> <li>ISO Boot Problems</li> <li>Netboot Problems</li> <li>Authentication Issues</li> <li>Script Execution Errors</li> </ul>"},{"location":"docs/TROUBLESHOOTING/#truenas-connectivity-issues","title":"TrueNAS Connectivity Issues","text":""},{"location":"docs/TROUBLESHOOTING/#problem-unable-to-connect-to-truenas-api","title":"Problem: Unable to connect to TrueNAS API","text":"<p>Symptoms: - Error message: \"Failed to connect to TrueNAS Scale: 401 Client Error: Unauthorized\" - Authentication failures in scripts</p> <p>Solutions: 1. Verify TrueNAS is accessible at the configured IP address:    <pre><code>ping 192.168.2.245\n</code></pre></p> <ol> <li> <p>Check if the TrueNAS web interface is accessible in a browser:    <pre><code>https://192.168.2.245:444\n</code></pre></p> </li> <li> <p>Verify API credentials:    <pre><code>./scripts/test_truenas_connection.py --host 192.168.2.245 --port 444\n</code></pre></p> </li> <li> <p>Regenerate API key in TrueNAS:</p> </li> <li>Log in to TrueNAS UI</li> <li>Navigate to Credentials \u2192 API Keys</li> <li> <p>Delete and re-create the key for OpenShift Multiboot</p> </li> <li> <p>Update your configuration:    <pre><code>./scripts/truenas_wrapper.sh  # Re-enter credentials\n</code></pre></p> </li> </ol>"},{"location":"docs/TROUBLESHOOTING/#problem-wrong-truenas-port-configured","title":"Problem: Wrong TrueNAS port configured","text":"<p>Symptoms: - Connection timeout or connection refused errors</p> <p>Solutions: 1. Verify the correct port in the TrueNAS web UI 2. Update your configuration:    <pre><code>./scripts/truenas_wrapper.sh  # Re-enter credentials with correct port\n</code></pre> 3. Use the port flag explicitly in commands:    <pre><code>./scripts/truenas_autodiscovery.py --host 192.168.2.245 --port 444\n</code></pre></p>"},{"location":"docs/TROUBLESHOOTING/#iscsi-boot-problems","title":"iSCSI Boot Problems","text":""},{"location":"docs/TROUBLESHOOTING/#problem-iscsi-targets-not-appearing-in-idrac","title":"Problem: iSCSI targets not appearing in iDRAC","text":"<p>Symptoms: - Unable to boot from iSCSI  - iSCSI targets not visible in boot options</p> <p>Solutions: 1. Verify target exists and is properly configured:    <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method iscsi --version 4.18 --check-only\n</code></pre></p> <ol> <li>Check TrueNAS iSCSI service status:</li> <li>Ensure the service is running in TrueNAS UI</li> <li> <p>Verify that allowed initiators include the server IQN or \"ALL\"</p> </li> <li> <p>Check network connectivity between server and TrueNAS:    <pre><code>ping 192.168.2.245\n</code></pre></p> </li> <li> <p>Manually reconfigure iSCSI initiator in iDRAC:</p> </li> <li>Log in to iDRAC web UI</li> <li>Navigate to Storage \u2192 iSCSI Initiator</li> <li>Configure target using values from <code>iscsi_targets.json</code></li> </ol>"},{"location":"docs/TROUBLESHOOTING/#problem-iscsi-boot-fails-with-no-boot-device-found","title":"Problem: iSCSI boot fails with \"No boot device found\"","text":"<p>Symptoms: - Server attempts to boot from iSCSI but fails - Error message \"No boot device found\" or similar</p> <p>Solutions: 1. Verify the zvol exists on TrueNAS and contains a bootable system 2. Check if iSCSI extent is properly linked to the target 3. Ensure boot order is correctly set in iDRAC 4. Verify CHAP settings (if used) match between initiator and target</p>"},{"location":"docs/TROUBLESHOOTING/#iso-boot-problems","title":"ISO Boot Problems","text":""},{"location":"docs/TROUBLESHOOTING/#problem-iso-not-available-or-accessible","title":"Problem: ISO not available or accessible","text":"<p>Symptoms: - Warning messages about ISO not being accessible - Boot fails when attempting ISO boot</p> <p>Solutions: 1. Check if ISO exists in TrueNAS:    <pre><code>./scripts/switch_openshift.py --server 192.168.2.230 --method iso --version 4.18 --check-only\n</code></pre></p> <ol> <li> <p>Regenerate the ISO:    <pre><code>./scripts/generate_openshift_iso.py --version 4.18 --rendezvous-ip 192.168.2.230\n</code></pre></p> </li> <li> <p>Verify NFS sharing is enabled in TrueNAS</p> </li> <li> <p>Check network connectivity and permissions</p> </li> </ol>"},{"location":"docs/TROUBLESHOOTING/#problem-iso-boot-starts-but-installation-fails","title":"Problem: ISO boot starts but installation fails","text":"<p>Symptoms: - Boot begins but OpenShift installation fails - Error messages during installation process</p> <p>Solutions: 1. Check that rendezvous IP is correctly configured 2. Verify pull secret is valid 3. Ensure network settings in agent-config.yaml are correct 4. Check logs during boot:    - Connect to the console via iDRAC    - Press tab during boot to see boot options    - Add <code>rd.break</code> to kernel parameters to examine initramfs</p>"},{"location":"docs/TROUBLESHOOTING/#netboot-problems","title":"Netboot Problems","text":""},{"location":"docs/TROUBLESHOOTING/#problem-netbootxyz-not-accessible","title":"Problem: netboot.xyz not accessible","text":"<p>Symptoms: - Warning about netboot.xyz not being accessible - Boot fails when attempting netboot</p> <p>Solutions: 1. Verify connectivity to netboot server:    <pre><code>curl -I https://netboot.omnisack.nl/ipxe/netboot.xyz.efi\n</code></pre></p> <ol> <li> <p>Set up a local netboot server if external service is unreliable</p> </li> <li> <p>Check iDRAC HTTP boot configuration:</p> </li> <li>Ensure HTTP boot is enabled in BIOS/UEFI</li> <li>Verify boot order includes HTTP boot</li> </ol>"},{"location":"docs/TROUBLESHOOTING/#problem-custom-menu-not-appearing","title":"Problem: Custom menu not appearing","text":"<p>Symptoms: - Default netboot menu shown instead of custom OpenShift menu - Unable to select OpenShift versions</p> <p>Solutions: 1. Regenerate custom menu:    <pre><code>./scripts/setup_netboot.py --truenas-ip 192.168.2.245\n</code></pre></p> <ol> <li>Verify custom menu URL is correctly set in switch_openshift.py</li> <li>Ensure the menu file is accessible via HTTP from the server</li> </ol>"},{"location":"docs/TROUBLESHOOTING/#authentication-issues","title":"Authentication Issues","text":""},{"location":"docs/TROUBLESHOOTING/#problem-api-key-authentication-fails","title":"Problem: API key authentication fails","text":"<p>Symptoms: - 401 Unauthorized errors - API requests failing</p> <p>Solutions: 1. Check API key permissions in TrueNAS 2. Regenerate API key and update configuration 3. Ensure API key is correctly formatted in auth.json</p>"},{"location":"docs/TROUBLESHOOTING/#problem-ssh-authentication-fails","title":"Problem: SSH authentication fails","text":"<p>Symptoms: - SSH connection refused - Permission denied errors</p> <p>Solutions: 1. Verify SSH service is enabled on TrueNAS 2. Check SSH key is correctly added to authorized_keys 3. Ensure permissions are correct on SSH files</p>"},{"location":"docs/TROUBLESHOOTING/#script-execution-errors","title":"Script Execution Errors","text":""},{"location":"docs/TROUBLESHOOTING/#problem-python-script-fails-to-execute","title":"Problem: Python script fails to execute","text":"<p>Symptoms: - \"Command not found\" or permission errors - Script crashes with import errors</p> <p>Solutions: 1. Verify Python is installed:    <pre><code>python --version\n</code></pre></p> <ol> <li> <p>Install required dependencies:    <pre><code>pip install requests pathlib\n</code></pre></p> </li> <li> <p>Make scripts executable:    <pre><code>chmod +x scripts/*.py\n</code></pre></p> </li> <li> <p>Check for syntax errors with linting:    <pre><code>flake8 scripts/problematic_script.py\n</code></pre></p> </li> </ol>"},{"location":"docs/TROUBLESHOOTING/#problem-finalize_deploymentsh-fails","title":"Problem: finalize_deployment.sh fails","text":"<p>Symptoms: - Deployment script stops with errors - Not all ISOs are generated</p> <p>Solutions: 1. Run with check-only first:    <pre><code>./scripts/finalize_deployment.sh  # Choose 'y' for check-only\n</code></pre></p> <ol> <li>Resolve specific errors reported in the log</li> <li>Re-run individual components manually:    <pre><code>./scripts/generate_openshift_iso.py --version 4.18 --rendezvous-ip 192.168.2.230\n./scripts/setup_netboot.py --truenas-ip 192.168.2.245\n</code></pre></li> </ol>"},{"location":"docs/TROUBLESHOOTING/#additional-support","title":"Additional Support","text":"<p>If you encounter issues not covered in this guide:</p> <ol> <li>Check the log files:</li> <li>Script logs: <code>*.log</code> in the current directory</li> <li>System logs: <code>journalctl -u iscsi*</code> on the server</li> <li> <p>TrueNAS logs: Available in the TrueNAS UI under System \u2192 Advanced \u2192 Logs</p> </li> <li> <p>Review the output of test scripts:    <pre><code>./scripts/test_setup.sh\n</code></pre></p> </li> <li> <p>For urgent issues, contact the system administrator at <code>admin@example.com</code></p> </li> </ol>"},{"location":"docs/TRUENAS_AUTHENTICATION/","title":"TrueNAS Authentication","text":""},{"location":"docs/TRUENAS_AUTHENTICATION/#truenas-scale-authentication-guide","title":"TrueNAS Scale Authentication Guide","text":"<p>This document explains the various methods for authenticating with TrueNAS Scale for administrative access, with a focus on the automation used in our OpenShift multiboot system.</p>"},{"location":"docs/TRUENAS_AUTHENTICATION/#authentication-methods","title":"Authentication Methods","text":"<p>TrueNAS Scale supports several authentication methods, each with its own advantages:</p> <ol> <li>Web UI Authentication: Interactive login using username/password</li> <li>SSH Authentication: Command-line access using SSH keys or passwords</li> <li>API Authentication: REST API access using either:</li> <li>Username/password (Basic Authentication)</li> <li>API keys (Bearer Authentication)</li> </ol> <p>Our scripts primarily use the REST API authentication methods, as they're the most suitable for automation.</p>"},{"location":"docs/TRUENAS_AUTHENTICATION/#setting-up-api-key-authentication-recommended","title":"Setting Up API Key Authentication (Recommended)","text":"<p>API keys provide a secure way to authenticate without storing passwords in scripts or exposing them in command-line arguments.</p>"},{"location":"docs/TRUENAS_AUTHENTICATION/#creating-an-api-key","title":"Creating an API Key","text":"<ol> <li>Log in to the TrueNAS Scale web interface (typically https://your-truenas-ip)</li> <li>Navigate to the user menu (top right) \u2192 \"My API Keys\"</li> <li>Click \"Add\"</li> <li>Provide the following information:</li> <li>Name: \"OpenShift Multiboot Automation\"</li> <li>Expiration date (optional): Set an expiration for security</li> <li>IP addresses (optional): Restrict to specific client IPs for added security</li> <li>Click \"Save\"</li> <li>Copy the generated API key and store it securely</li> </ol>"},{"location":"docs/TRUENAS_AUTHENTICATION/#api-documentation","title":"API Documentation","text":"<p>The TrueNAS SCALE API documentation is available at: - https://your-truenas-ip/api/docs/</p> <p>You can use this documentation to explore available endpoints and test API calls directly.</p>"},{"location":"docs/TRUENAS_AUTHENTICATION/#using-the-api-key-with-our-scripts","title":"Using the API Key with Our Scripts","text":"<p>Once you have an API key, you can use it with the <code>truenas_autodiscovery.py</code> script:</p> <pre><code>./scripts/truenas_autodiscovery.py --host 192.168.2.245 --api-key \"YOUR_API_KEY_HERE\"\n</code></pre>"},{"location":"docs/TRUENAS_AUTHENTICATION/#usernamepassword-authentication","title":"Username/Password Authentication","text":"<p>While less secure than API key authentication, username/password can also be used:</p> <pre><code># Provide password on the command line (not recommended)\n./scripts/truenas_autodiscovery.py --host 192.168.2.245 --username root --password \"YOUR_PASSWORD\"\n\n# Prompt for password (more secure)\n./scripts/truenas_autodiscovery.py --host 192.168.2.245 --username root\n</code></pre>"},{"location":"docs/TRUENAS_AUTHENTICATION/#creating-a-secure-authentication-file","title":"Creating a Secure Authentication File","text":"<p>For convenience and security, you can create a configuration file for authentication:</p> <ol> <li>Create a file in a secure location (with restricted permissions):</li> </ol> <pre><code>mkdir -p ~/.config/truenas\ntouch ~/.config/truenas/auth.json\nchmod 600 ~/.config/truenas/auth.json\n</code></pre> <ol> <li>Add your authentication information:</li> </ol> <pre><code>{\n  \"host\": \"192.168.2.245\",\n  \"api_key\": \"YOUR_API_KEY_HERE\"\n}\n</code></pre> <ol> <li>Create a wrapper script to use this configuration:</li> </ol> <pre><code>#!/bin/bash\n# truenas_wrapper.sh\n\nAUTH_FILE=\"$HOME/.config/truenas/auth.json\"\n\nif [ ! -f \"$AUTH_FILE\" ]; then\n  echo \"Authentication file not found: $AUTH_FILE\"\n  exit 1\nfi\n\n# Extract host and API key from JSON\nHOST=$(jq -r '.host' &lt; \"$AUTH_FILE\")\nAPI_KEY=$(jq -r '.api_key' &lt; \"$AUTH_FILE\")\n\n# Pass to the actual script\n./scripts/truenas_autodiscovery.py --host \"$HOST\" --api-key \"$API_KEY\" \"$@\"\n</code></pre>"},{"location":"docs/TRUENAS_AUTHENTICATION/#ssh-authentication-for-remote-commands","title":"SSH Authentication for Remote Commands","text":"<p>The <code>setup_truenas.sh</code> script needs to be run directly on the TrueNAS Scale server, which requires SSH access:</p> <ol> <li> <p>Generate an SSH key pair (if you haven't already):    <pre><code>ssh-keygen -t ed25519 -C \"openshift-multiboot\"\n</code></pre></p> </li> <li> <p>Copy your public key to TrueNAS Scale:    <pre><code>ssh-copy-id root@192.168.2.245\n</code></pre></p> </li> <li> <p>Verify SSH access:    <pre><code>ssh root@192.168.2.245 \"uname -a\"\n</code></pre></p> </li> </ol>"},{"location":"docs/TRUENAS_AUTHENTICATION/#port-and-protocol-configuration","title":"Port and Protocol Configuration","text":"<p>TrueNAS SCALE typically serves its web interface and API on the following ports:</p> <ul> <li>HTTP: Port 80 (default, insecure)</li> <li>HTTPS: Port 443 (default, secure)</li> </ul> <p>However, some installations may use custom port configurations:</p> <ul> <li>Our specific installation uses HTTPS on port 444</li> <li>You can use the <code>scripts/test_truenas_connection.py</code> script to detect the correct settings:   <pre><code>./scripts/test_truenas_connection.py --host 192.168.2.245 --api-key \"YOUR_API_KEY\"\n</code></pre></li> </ul>"},{"location":"docs/TRUENAS_AUTHENTICATION/#admin-access-best-practices","title":"Admin Access Best Practices","text":"<ol> <li>Use API keys instead of username/password authentication</li> <li>Rotate API keys periodically (every 90 days)</li> <li>Set expiration dates for API keys to limit their validity period</li> <li>Use a dedicated user account instead of <code>root</code> when possible</li> <li>Enable 2FA for web UI access</li> <li>Use SSH keys instead of passwords for SSH access</li> <li>Store credentials securely (encrypted when at rest)</li> <li>Use restrictive file permissions on any files containing credentials</li> <li>Keep credentials out of version control (use the .gitignore file)</li> <li>Restrict API keys by IP address when possible</li> </ol>"},{"location":"docs/TRUENAS_AUTHENTICATION/#credential-handling-and-version-control","title":"Credential Handling and Version Control","text":"<p>The repository includes a <code>.gitignore</code> file configured to prevent sensitive data from being committed to version control:</p> <pre><code># TrueNAS authentication files\n.truenas_auth\n.truenas_config\n*auth.json\n*credentials.json\n.config/truenas/\n\n# API keys and sensitive data\n*.key\n*.pem\n*.crt\n*api_key*\n*apikey*\n*secret*\n*password*\n*credential*\n</code></pre> <p>When using the <code>truenas_wrapper.sh</code> script, your credentials will be stored in <code>~/.config/truenas/auth.json</code>, which is automatically excluded from git. This ensures your sensitive information stays secure and local to your machine.</p>"},{"location":"docs/TRUENAS_AUTHENTICATION/#recommended-workflow-for-teams","title":"Recommended Workflow for Teams","text":"<p>If working with a team:</p> <ol> <li>Never commit real credentials to the repository</li> <li>Use environment variables or secure credential storage whenever possible</li> <li>Provide example configuration files with placeholders instead of real values</li> <li>Document the credential setup process for other team members</li> </ol>"},{"location":"docs/TRUENAS_AUTHENTICATION/#credential-management-in-scripts","title":"Credential Management in Scripts","text":"<p>Our implementation handles credentials in the following ways:</p> <ol> <li>No hardcoded credentials in any script</li> <li>Password prompting when not provided as arguments</li> <li>API key support for secure, non-interactive authentication</li> <li>SSH key authentication for remote commands</li> </ol>"},{"location":"docs/TRUENAS_AUTHENTICATION/#testing-authentication","title":"Testing Authentication","text":"<p>To verify your authentication is working correctly:</p> <pre><code># Test API authentication with the autodiscovery script\n./scripts/truenas_autodiscovery.py --host 192.168.2.245 --discover-only\n\n# Test SSH authentication\nssh root@192.168.2.245 \"uname -a\"\n</code></pre>"},{"location":"docs/TRUENAS_AUTHENTICATION/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter authentication issues:</p> <ol> <li>API key errors:</li> <li>Verify the API key is correct and not expired</li> <li> <p>Ensure the API key has the necessary permissions</p> </li> <li> <p>SSH access issues:</p> </li> <li>Check SSH service is running on TrueNAS (<code>systemctl status ssh</code>)</li> <li>Verify your SSH key is correctly added to authorized_keys</li> <li>Check file permissions on TrueNAS for SSH files</li> <li> <p>Ensure SSH is allowed through any firewalls</p> </li> <li> <p>Permission denied errors:</p> </li> <li>Verify you're using an account with administrative privileges</li> <li>Check the API key permissions on TrueNAS Scale</li> </ol>"},{"location":"docs/TRUENAS_ISCSI_FINDING_BLOCK_DEVICES_USE_OCP_AGENT_BASED/","title":"integrate this with what we learned from identifien the device from truenas scale","text":"<p>To integrate the process of identifying iSCSI devices from TrueNAS Scale with a Single Node OpenShift (SNO) agent-based installation, follow these steps to ensure reliable identification and configuration of the root disk. This approach combines persistent disk identification methods and TrueNAS Scale iSCSI setup principles.</p>"},{"location":"docs/TRUENAS_ISCSI_FINDING_BLOCK_DEVICES_USE_OCP_AGENT_BASED/#steps-to-configure-iscsi-disk-for-sno-installation","title":"Steps to Configure iSCSI Disk for SNO Installation","text":""},{"location":"docs/TRUENAS_ISCSI_FINDING_BLOCK_DEVICES_USE_OCP_AGENT_BASED/#1-configure-iscsi-on-truenas-scale","title":"1. Configure iSCSI on TrueNAS Scale","text":"<p>Set up an iSCSI target and extent on TrueNAS Scale to serve as the boot disk for your SNO installation:</p> <ol> <li>Create a ZFS Dataset or Zvol:<ul> <li>If using ZFS, create a dedicated dataset for the root disk.</li> <li>For block storage, create a Zvol under your dataset (ensure it is correctly configured as an extent).</li> </ul> </li> <li>Set Up iSCSI Block Share:<ul> <li>Use the TrueNAS Scale wizard or manual configuration:<ul> <li>Extent Type: Select \"Device\" if using a full disk or Zvol.</li> <li>Target IQN: Create a unique IQN (e.g., <code>iqn.2025-04.com.example:boot-disk</code>).</li> <li>Portal Configuration: Add the IP address of your TrueNAS server.</li> </ul> </li> </ul> </li> <li>Record Persistent Identifiers:<ul> <li>Note the LUN ID, IQN, and portal IP address for use in Red Hat-based systems.</li> </ul> </li> </ol>"},{"location":"docs/TRUENAS_ISCSI_FINDING_BLOCK_DEVICES_USE_OCP_AGENT_BASED/#2-discover-and-identify-the-iscsi-disk-on-red-hat-systems","title":"2. Discover and Identify the iSCSI Disk on Red Hat Systems","text":"<p>After configuring TrueNAS Scale, discover the iSCSI device on your Red Hat system using persistent attributes:</p> <ol> <li>Login to the iSCSI Target: Use <code>iscsiadm</code> to discover and log in to the target:</li> </ol> <pre><code>iscsiadm -m discovery -t sendtargets -p &amp;lt;TrueNAS_IP&amp;gt;\niscsiadm -m node --login\n</code></pre> <ol> <li>Verify Disk Mapping: Check which local block device corresponds to the iSCSI LUN:</li> </ol> <pre><code>lsblk\niscsiadm --mode session -P 3\n</code></pre> <p>This will display detailed mappings, including LUNs and their associated local devices (e.g., <code>/dev/sda</code>). 3. Retrieve Persistent Attributes: Use <code>lsblk</code> or <code>udevadm</code> to gather persistent identifiers like WWN, serial number, or model:</p> <pre><code>lsblk -o NAME,WWN,MODEL,SERIAL\nudevadm info --query=all --name=/dev/sda | grep ID_\n</code></pre>"},{"location":"docs/TRUENAS_ISCSI_FINDING_BLOCK_DEVICES_USE_OCP_AGENT_BASED/#3-configure-agent-based-installation-with-persistent-disk-hints","title":"3. Configure Agent-Based Installation with Persistent Disk Hints","text":"<p>Update your SNO agent configuration (<code>agent-config.yaml</code>) to ensure the installer uses the correct disk:</p>"},{"location":"docs/TRUENAS_ISCSI_FINDING_BLOCK_DEVICES_USE_OCP_AGENT_BASED/#example-configuration","title":"Example Configuration:","text":"<pre><code>hosts:\n  - hostname: master-0\n    rootDeviceHints:\n      wwn: \"0x50014ee2bae7ffa5\"  # Replace with actual WWN\n      serialNumber: \"BTHV507002P71P6GN\"  # Replace with actual serial number\n</code></pre>"},{"location":"docs/TRUENAS_ISCSI_FINDING_BLOCK_DEVICES_USE_OCP_AGENT_BASED/#steps","title":"Steps:","text":"<ol> <li>Replace <code>/dev/sda</code> with persistent identifiers like WWN or serial number.</li> <li>Validate that these identifiers match the iSCSI device discovered earlier.</li> </ol>"},{"location":"docs/TRUENAS_ISCSI_FINDING_BLOCK_DEVICES_USE_OCP_AGENT_BASED/#4-integrate-automation-scripts","title":"4. Integrate Automation Scripts","text":"<p>In your automation scripts for creating volumes on TrueNAS Scale and configuring SNO installations:</p> <ol> <li>Automate Volume Creation: Use TrueNAS APIs or CLI commands to create Zvols and configure iSCSI shares dynamically.</li> <li>Automate Disk Discovery on Red Hat Systems: Run <code>iscsiadm</code> commands in your script to discover targets and retrieve persistent attributes automatically.</li> <li>Update Agent Configuration Dynamically: Parse output from <code>lsblk</code> or <code>udevadm</code> to populate <code>rootDeviceHints</code> in <code>agent-config.yaml</code>.</li> </ol>"},{"location":"docs/TRUENAS_ISCSI_FINDING_BLOCK_DEVICES_USE_OCP_AGENT_BASED/#5-verify-installation","title":"5. Verify Installation","text":"<p>After configuring the agent-based installation:</p> <ol> <li>Check that the installer uses the correct disk by inspecting logs for errors like <code>WARNING BootstrapInPlace.InstallationDisk: /dev/sda is ignored</code>.</li> <li>Validate that <code>/dev/disk/by-path/</code> or <code>/dev/disk/by-uuid/</code> points to the correct iSCSI device.</li> </ol>"},{"location":"docs/TRUENAS_ISCSI_FINDING_BLOCK_DEVICES_USE_OCP_AGENT_BASED/#best-practices","title":"Best Practices","text":"<ol> <li>Use persistent identifiers (WWN, serial number) instead of transient names like <code>/dev/sda</code>.</li> <li>Ensure LUN IDs are unique across targets in TrueNAS Scale.</li> <li>Regularly test automation scripts in isolated environments before deploying them in production.</li> </ol> <p>By combining persistent disk identification methods with robust TrueNAS Scale configurations, you can reliably integrate SAN boot processes into your SNO installations without encountering device mapping issues.</p> \u2042 <ol> <li> <p>https://forum.proxmox.com/threads/proxmox-ve-and-zfs-over-iscsi-on-truenas-scale-my-steps-to-make-it-work.125387/\u00a0\u21a9</p> </li> <li> <p>https://www.reddit.com/r/truenas/comments/pqk4b3/corescale_as_iscsi_initiator/\u00a0\u21a9</p> </li> <li> <p>https://www.truenas.com/community/threads/adding-an-external-iscsi-share-to-truenas-scale.108988/\u00a0\u21a9</p> </li> <li> <p>https://www.reddit.com/r/homelab/comments/uiwqdn/what_am_i_missing_iscsi_between_truenas_scale_and/\u00a0\u21a9</p> </li> <li> <p>https://www.truenas.com/docs/scale/scaletutorials/shares/iscsi/addingiscsishares/\u00a0\u21a9</p> </li> <li> <p>https://www.truenas.com/community/threads/truenas-scale-22-iscsi-device-extend-not-show-device.104740/\u00a0\u21a9</p> </li> <li> <p>https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/managing_storage_devices/configuring-an-iscsi-initiator_managing-storage-devices\u00a0\u21a9</p> </li> <li> <p>https://jonathangazeley.com/2021/01/05/using-truenas-to-provide-persistent-storage-for-kubernetes/\u00a0\u21a9</p> </li> </ol>"}]}